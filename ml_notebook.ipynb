{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys, os, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#classifier models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'./data/features_3_sec.csv')\n",
    "df = df.drop(labels='filename', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = LabelEncoder().fit_transform(df.iloc[:,-1])\n",
    "X = StandardScaler().fit_transform(np.array(df.iloc[:,:-1], dtype=float))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, epochs, optimizer):\n",
    "    batch_size = 128\n",
    "    #callback = myCallback()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=['accuracy'])\n",
    "    return model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                     epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "def plotValidate(history):\n",
    "    print(\"Validation Accuracy\", max(history.history['val_accuracy']))\n",
    "    pd.DataFrame(history.history).plot(figsize=(12,6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "units, rate = 64, 0.2\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(units*2**3, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dropout(rate),\n",
    "    \n",
    "    keras.layers.Dense(units*2**2, activation='relu'),\n",
    "    keras.layers.Dropout(rate),\n",
    "    \n",
    "    keras.layers.Dense(units*2**1, activation='relu'),\n",
    "    keras.layers.Dropout(rate),\n",
    "    \n",
    "    keras.layers.Dense(units, activation='relu'),\n",
    "    keras.layers.Dropout(rate),\n",
    "    \n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               30208     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 203,338\n",
      "Trainable params: 203,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6693 samples, validate on 3297 samples\n",
      "Epoch 1/600\n",
      "6693/6693 [==============================] - 2s 330us/sample - loss: 1.6807 - acc: 0.3919 - val_loss: 1.1625 - val_acc: 0.5778\n",
      "Epoch 2/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 1.1691 - acc: 0.5858 - val_loss: 0.8772 - val_acc: 0.7006\n",
      "Epoch 3/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.9396 - acc: 0.6827 - val_loss: 0.7518 - val_acc: 0.7389\n",
      "Epoch 4/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.8081 - acc: 0.7222 - val_loss: 0.6769 - val_acc: 0.7658\n",
      "Epoch 5/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.7052 - acc: 0.7662 - val_loss: 0.5852 - val_acc: 0.7974\n",
      "Epoch 6/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.6456 - acc: 0.7844 - val_loss: 0.5593 - val_acc: 0.8083\n",
      "Epoch 7/600\n",
      "6693/6693 [==============================] - 0s 71us/sample - loss: 0.5630 - acc: 0.8126 - val_loss: 0.5354 - val_acc: 0.8153\n",
      "Epoch 8/600\n",
      "6693/6693 [==============================] - 0s 71us/sample - loss: 0.5123 - acc: 0.8288 - val_loss: 0.5151 - val_acc: 0.8253\n",
      "Epoch 9/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.4547 - acc: 0.8452 - val_loss: 0.4738 - val_acc: 0.8426\n",
      "Epoch 10/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.4127 - acc: 0.8631 - val_loss: 0.4452 - val_acc: 0.8490\n",
      "Epoch 11/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.3785 - acc: 0.8732 - val_loss: 0.4377 - val_acc: 0.8526\n",
      "Epoch 12/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.3427 - acc: 0.8856 - val_loss: 0.4214 - val_acc: 0.8681\n",
      "Epoch 13/600\n",
      "6693/6693 [==============================] - 1s 82us/sample - loss: 0.3201 - acc: 0.8935 - val_loss: 0.4228 - val_acc: 0.8514\n",
      "Epoch 14/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.2890 - acc: 0.9029 - val_loss: 0.3732 - val_acc: 0.8826\n",
      "Epoch 15/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.2644 - acc: 0.9156 - val_loss: 0.3788 - val_acc: 0.8793\n",
      "Epoch 16/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.2491 - acc: 0.9165 - val_loss: 0.3898 - val_acc: 0.8802\n",
      "Epoch 17/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.2351 - acc: 0.9241 - val_loss: 0.3666 - val_acc: 0.8838\n",
      "Epoch 18/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.2124 - acc: 0.9302 - val_loss: 0.3687 - val_acc: 0.8872\n",
      "Epoch 19/600\n",
      "6693/6693 [==============================] - 1s 75us/sample - loss: 0.1959 - acc: 0.9359 - val_loss: 0.3627 - val_acc: 0.8896\n",
      "Epoch 20/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.1804 - acc: 0.9392 - val_loss: 0.3479 - val_acc: 0.8899\n",
      "Epoch 21/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.1731 - acc: 0.9422 - val_loss: 0.3362 - val_acc: 0.8963\n",
      "Epoch 22/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.1661 - acc: 0.9440 - val_loss: 0.3499 - val_acc: 0.8963\n",
      "Epoch 23/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.1540 - acc: 0.9505 - val_loss: 0.3513 - val_acc: 0.8911\n",
      "Epoch 24/600\n",
      "6693/6693 [==============================] - 0s 75us/sample - loss: 0.1455 - acc: 0.9513 - val_loss: 0.3474 - val_acc: 0.9032\n",
      "Epoch 25/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.1432 - acc: 0.9552 - val_loss: 0.3528 - val_acc: 0.8944\n",
      "Epoch 26/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.1247 - acc: 0.9568 - val_loss: 0.3725 - val_acc: 0.8951\n",
      "Epoch 27/600\n",
      "6693/6693 [==============================] - 1s 81us/sample - loss: 0.1239 - acc: 0.9628 - val_loss: 0.3549 - val_acc: 0.8984\n",
      "Epoch 28/600\n",
      "6693/6693 [==============================] - 0s 75us/sample - loss: 0.1291 - acc: 0.9610 - val_loss: 0.3627 - val_acc: 0.8981\n",
      "Epoch 29/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.1151 - acc: 0.9616 - val_loss: 0.3703 - val_acc: 0.9011\n",
      "Epoch 30/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.1075 - acc: 0.9631 - val_loss: 0.3695 - val_acc: 0.8975\n",
      "Epoch 31/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.1112 - acc: 0.9626 - val_loss: 0.3761 - val_acc: 0.8996\n",
      "Epoch 32/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.1045 - acc: 0.9649 - val_loss: 0.3917 - val_acc: 0.8932\n",
      "Epoch 33/600\n",
      "6693/6693 [==============================] - 1s 75us/sample - loss: 0.1096 - acc: 0.9629 - val_loss: 0.3674 - val_acc: 0.8966\n",
      "Epoch 34/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.1075 - acc: 0.9637 - val_loss: 0.3495 - val_acc: 0.9020\n",
      "Epoch 35/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.1034 - acc: 0.9679 - val_loss: 0.3711 - val_acc: 0.8999\n",
      "Epoch 36/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.0954 - acc: 0.9710 - val_loss: 0.3480 - val_acc: 0.9060\n",
      "Epoch 37/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.0862 - acc: 0.9736 - val_loss: 0.3406 - val_acc: 0.9066\n",
      "Epoch 38/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.0799 - acc: 0.9740 - val_loss: 0.3557 - val_acc: 0.9045\n",
      "Epoch 39/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.0757 - acc: 0.9750 - val_loss: 0.3388 - val_acc: 0.9066\n",
      "Epoch 40/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.0743 - acc: 0.9779 - val_loss: 0.3379 - val_acc: 0.9126\n",
      "Epoch 41/600\n",
      "6693/6693 [==============================] - 0s 71us/sample - loss: 0.0691 - acc: 0.9767 - val_loss: 0.3599 - val_acc: 0.9057\n",
      "Epoch 42/600\n",
      "6693/6693 [==============================] - 1s 75us/sample - loss: 0.0626 - acc: 0.9791 - val_loss: 0.3454 - val_acc: 0.9054\n",
      "Epoch 43/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.0765 - acc: 0.9761 - val_loss: 0.3810 - val_acc: 0.9035\n",
      "Epoch 44/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.0866 - acc: 0.9728 - val_loss: 0.3734 - val_acc: 0.8990\n",
      "Epoch 45/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.0677 - acc: 0.9788 - val_loss: 0.3554 - val_acc: 0.9093\n",
      "Epoch 46/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.0628 - acc: 0.9800 - val_loss: 0.3727 - val_acc: 0.9026\n",
      "Epoch 47/600\n",
      "6693/6693 [==============================] - 0s 75us/sample - loss: 0.0586 - acc: 0.9806 - val_loss: 0.3743 - val_acc: 0.9111\n",
      "Epoch 48/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.0701 - acc: 0.9758 - val_loss: 0.3421 - val_acc: 0.9166\n",
      "Epoch 49/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.0647 - acc: 0.9789 - val_loss: 0.3566 - val_acc: 0.9114\n",
      "Epoch 50/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.0702 - acc: 0.9779 - val_loss: 0.3384 - val_acc: 0.9139\n",
      "Epoch 51/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0648 - acc: 0.9789 - val_loss: 0.3591 - val_acc: 0.9151\n",
      "Epoch 52/600\n",
      "6693/6693 [==============================] - 0s 70us/sample - loss: 0.0556 - acc: 0.9819 - val_loss: 0.3527 - val_acc: 0.9102\n",
      "Epoch 53/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.0518 - acc: 0.9831 - val_loss: 0.3679 - val_acc: 0.9093\n",
      "Epoch 54/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.0569 - acc: 0.9804 - val_loss: 0.3478 - val_acc: 0.9175\n",
      "Epoch 55/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.0621 - acc: 0.9783 - val_loss: 0.3679 - val_acc: 0.9093\n",
      "Epoch 56/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.0523 - acc: 0.9842 - val_loss: 0.3746 - val_acc: 0.9136\n",
      "Epoch 57/600\n",
      "6693/6693 [==============================] - 0s 72us/sample - loss: 0.0628 - acc: 0.9788 - val_loss: 0.3581 - val_acc: 0.9099\n",
      "Epoch 58/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.0582 - acc: 0.9839 - val_loss: 0.3385 - val_acc: 0.9142\n",
      "Epoch 59/600\n",
      "6693/6693 [==============================] - 1s 78us/sample - loss: 0.0584 - acc: 0.9816 - val_loss: 0.3736 - val_acc: 0.9133\n",
      "Epoch 60/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.0554 - acc: 0.9837 - val_loss: 0.3528 - val_acc: 0.9148\n",
      "Epoch 61/600\n",
      "6693/6693 [==============================] - 0s 73us/sample - loss: 0.0582 - acc: 0.9807 - val_loss: 0.3627 - val_acc: 0.9090\n",
      "Epoch 62/600\n",
      "6693/6693 [==============================] - 0s 70us/sample - loss: 0.0474 - acc: 0.9860 - val_loss: 0.3639 - val_acc: 0.9108\n",
      "Epoch 63/600\n",
      "6693/6693 [==============================] - 0s 70us/sample - loss: 0.0476 - acc: 0.9839 - val_loss: 0.3721 - val_acc: 0.9069\n",
      "Epoch 64/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.0508 - acc: 0.9849 - val_loss: 0.3532 - val_acc: 0.9136\n",
      "Epoch 65/600\n",
      "6693/6693 [==============================] - 1s 76us/sample - loss: 0.0432 - acc: 0.9886 - val_loss: 0.3563 - val_acc: 0.9142\n",
      "Epoch 66/600\n",
      "6693/6693 [==============================] - 0s 74us/sample - loss: 0.0393 - acc: 0.9883 - val_loss: 0.3791 - val_acc: 0.9130\n",
      "Epoch 67/600\n",
      "6693/6693 [==============================] - 1s 75us/sample - loss: 0.0499 - acc: 0.9852 - val_loss: 0.3655 - val_acc: 0.9087\n",
      "Epoch 68/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0467 - acc: 0.9839 - val_loss: 0.3652 - val_acc: 0.9160\n",
      "Epoch 69/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0469 - acc: 0.9837 - val_loss: 0.3785 - val_acc: 0.9117\n",
      "Epoch 70/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0404 - acc: 0.9867 - val_loss: 0.3920 - val_acc: 0.9099\n",
      "Epoch 71/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0389 - acc: 0.9864 - val_loss: 0.4225 - val_acc: 0.9045\n",
      "Epoch 72/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0490 - acc: 0.9873 - val_loss: 0.3811 - val_acc: 0.9154\n",
      "Epoch 73/600\n",
      "6693/6693 [==============================] - 1s 110us/sample - loss: 0.0457 - acc: 0.9863 - val_loss: 0.3755 - val_acc: 0.9169\n",
      "Epoch 74/600\n",
      "6693/6693 [==============================] - 1s 94us/sample - loss: 0.0397 - acc: 0.9870 - val_loss: 0.3808 - val_acc: 0.9123\n",
      "Epoch 75/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0377 - acc: 0.9872 - val_loss: 0.3766 - val_acc: 0.9133\n",
      "Epoch 76/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0380 - acc: 0.9876 - val_loss: 0.3823 - val_acc: 0.9145\n",
      "Epoch 77/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0431 - acc: 0.9851 - val_loss: 0.3838 - val_acc: 0.9114\n",
      "Epoch 78/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0449 - acc: 0.9867 - val_loss: 0.3794 - val_acc: 0.9126\n",
      "Epoch 79/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0378 - acc: 0.9873 - val_loss: 0.4192 - val_acc: 0.9084\n",
      "Epoch 80/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0457 - acc: 0.9880 - val_loss: 0.4103 - val_acc: 0.9120\n",
      "Epoch 81/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0391 - acc: 0.9870 - val_loss: 0.4354 - val_acc: 0.9032\n",
      "Epoch 82/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0432 - acc: 0.9869 - val_loss: 0.4205 - val_acc: 0.9111\n",
      "Epoch 83/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0499 - acc: 0.9840 - val_loss: 0.3401 - val_acc: 0.9142\n",
      "Epoch 84/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0416 - acc: 0.9873 - val_loss: 0.3767 - val_acc: 0.9157\n",
      "Epoch 85/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0395 - acc: 0.9869 - val_loss: 0.3772 - val_acc: 0.9133\n",
      "Epoch 86/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0514 - acc: 0.9852 - val_loss: 0.3798 - val_acc: 0.9111\n",
      "Epoch 87/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0436 - acc: 0.9867 - val_loss: 0.3918 - val_acc: 0.9075\n",
      "Epoch 88/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0309 - acc: 0.9891 - val_loss: 0.4169 - val_acc: 0.9102\n",
      "Epoch 89/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0491 - acc: 0.9845 - val_loss: 0.3888 - val_acc: 0.9102\n",
      "Epoch 90/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0412 - acc: 0.9869 - val_loss: 0.3857 - val_acc: 0.9078\n",
      "Epoch 91/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0400 - acc: 0.9866 - val_loss: 0.3778 - val_acc: 0.9130\n",
      "Epoch 92/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0349 - acc: 0.9880 - val_loss: 0.3882 - val_acc: 0.9142\n",
      "Epoch 93/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0455 - acc: 0.9852 - val_loss: 0.3840 - val_acc: 0.9130\n",
      "Epoch 94/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0415 - acc: 0.9867 - val_loss: 0.3597 - val_acc: 0.9196\n",
      "Epoch 95/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0280 - acc: 0.9919 - val_loss: 0.3856 - val_acc: 0.9126\n",
      "Epoch 96/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0288 - acc: 0.9913 - val_loss: 0.3620 - val_acc: 0.9172\n",
      "Epoch 97/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0382 - acc: 0.9879 - val_loss: 0.3702 - val_acc: 0.9136\n",
      "Epoch 98/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0380 - acc: 0.9877 - val_loss: 0.4018 - val_acc: 0.9139\n",
      "Epoch 99/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0354 - acc: 0.9882 - val_loss: 0.3574 - val_acc: 0.9139\n",
      "Epoch 100/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0396 - acc: 0.9866 - val_loss: 0.3848 - val_acc: 0.9130\n",
      "Epoch 101/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0342 - acc: 0.9904 - val_loss: 0.3866 - val_acc: 0.9148\n",
      "Epoch 102/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0411 - acc: 0.9870 - val_loss: 0.3816 - val_acc: 0.9163\n",
      "Epoch 103/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0420 - acc: 0.9867 - val_loss: 0.3735 - val_acc: 0.9157\n",
      "Epoch 104/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0352 - acc: 0.9877 - val_loss: 0.3567 - val_acc: 0.9233\n",
      "Epoch 105/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0321 - acc: 0.9885 - val_loss: 0.3596 - val_acc: 0.9190\n",
      "Epoch 106/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0297 - acc: 0.9895 - val_loss: 0.3707 - val_acc: 0.9205\n",
      "Epoch 107/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0275 - acc: 0.9892 - val_loss: 0.3959 - val_acc: 0.9169\n",
      "Epoch 108/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0314 - acc: 0.9888 - val_loss: 0.3803 - val_acc: 0.9211\n",
      "Epoch 109/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0298 - acc: 0.9906 - val_loss: 0.3750 - val_acc: 0.9202\n",
      "Epoch 110/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0326 - acc: 0.9898 - val_loss: 0.4001 - val_acc: 0.9151\n",
      "Epoch 111/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0411 - acc: 0.9861 - val_loss: 0.3906 - val_acc: 0.9136\n",
      "Epoch 112/600\n",
      "6693/6693 [==============================] - 1s 110us/sample - loss: 0.0425 - acc: 0.9876 - val_loss: 0.3673 - val_acc: 0.9151\n",
      "Epoch 113/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0423 - acc: 0.9880 - val_loss: 0.3717 - val_acc: 0.9105\n",
      "Epoch 114/600\n",
      "6693/6693 [==============================] - 1s 91us/sample - loss: 0.0306 - acc: 0.9904 - val_loss: 0.3734 - val_acc: 0.9202\n",
      "Epoch 115/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0286 - acc: 0.9897 - val_loss: 0.3741 - val_acc: 0.9199\n",
      "Epoch 116/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0179 - acc: 0.9951 - val_loss: 0.3876 - val_acc: 0.9224\n",
      "Epoch 117/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0247 - acc: 0.9921 - val_loss: 0.4151 - val_acc: 0.9139\n",
      "Epoch 118/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0214 - acc: 0.9934 - val_loss: 0.4065 - val_acc: 0.9166\n",
      "Epoch 119/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0177 - acc: 0.9936 - val_loss: 0.4105 - val_acc: 0.9217\n",
      "Epoch 120/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0269 - acc: 0.9913 - val_loss: 0.4310 - val_acc: 0.9148\n",
      "Epoch 121/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0322 - acc: 0.9894 - val_loss: 0.4307 - val_acc: 0.9111\n",
      "Epoch 122/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0412 - acc: 0.9882 - val_loss: 0.3812 - val_acc: 0.9166\n",
      "Epoch 123/600\n",
      "6693/6693 [==============================] - 1s 92us/sample - loss: 0.0362 - acc: 0.9888 - val_loss: 0.3819 - val_acc: 0.9175\n",
      "Epoch 124/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0239 - acc: 0.9919 - val_loss: 0.4285 - val_acc: 0.9108\n",
      "Epoch 125/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0339 - acc: 0.9894 - val_loss: 0.4071 - val_acc: 0.9136\n",
      "Epoch 126/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0368 - acc: 0.9892 - val_loss: 0.4085 - val_acc: 0.9139\n",
      "Epoch 127/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0322 - acc: 0.9915 - val_loss: 0.3800 - val_acc: 0.9184\n",
      "Epoch 128/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0298 - acc: 0.9913 - val_loss: 0.3929 - val_acc: 0.9142\n",
      "Epoch 129/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0313 - acc: 0.9907 - val_loss: 0.3977 - val_acc: 0.9169\n",
      "Epoch 130/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0271 - acc: 0.9915 - val_loss: 0.3960 - val_acc: 0.9190\n",
      "Epoch 131/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0342 - acc: 0.9889 - val_loss: 0.4134 - val_acc: 0.9175\n",
      "Epoch 132/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0300 - acc: 0.9906 - val_loss: 0.3940 - val_acc: 0.9169\n",
      "Epoch 133/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0360 - acc: 0.9882 - val_loss: 0.3918 - val_acc: 0.9187\n",
      "Epoch 134/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0344 - acc: 0.9883 - val_loss: 0.4132 - val_acc: 0.9123\n",
      "Epoch 135/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0333 - acc: 0.9895 - val_loss: 0.3775 - val_acc: 0.9172\n",
      "Epoch 136/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0247 - acc: 0.9919 - val_loss: 0.4085 - val_acc: 0.9181\n",
      "Epoch 137/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0349 - acc: 0.9892 - val_loss: 0.3428 - val_acc: 0.9217\n",
      "Epoch 138/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0198 - acc: 0.9936 - val_loss: 0.3656 - val_acc: 0.9181\n",
      "Epoch 139/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0225 - acc: 0.9925 - val_loss: 0.3813 - val_acc: 0.9214\n",
      "Epoch 140/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0260 - acc: 0.9921 - val_loss: 0.3998 - val_acc: 0.9184\n",
      "Epoch 141/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0233 - acc: 0.9930 - val_loss: 0.3765 - val_acc: 0.9281\n",
      "Epoch 142/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0203 - acc: 0.9939 - val_loss: 0.3732 - val_acc: 0.9245\n",
      "Epoch 143/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0268 - acc: 0.9922 - val_loss: 0.4215 - val_acc: 0.9190\n",
      "Epoch 144/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0302 - acc: 0.9903 - val_loss: 0.4456 - val_acc: 0.9130\n",
      "Epoch 145/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0393 - acc: 0.9888 - val_loss: 0.3839 - val_acc: 0.9151\n",
      "Epoch 146/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0361 - acc: 0.9894 - val_loss: 0.3828 - val_acc: 0.9202\n",
      "Epoch 147/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0277 - acc: 0.9915 - val_loss: 0.3729 - val_acc: 0.9248\n",
      "Epoch 148/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0294 - acc: 0.9910 - val_loss: 0.3513 - val_acc: 0.9266\n",
      "Epoch 149/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0372 - acc: 0.9900 - val_loss: 0.3886 - val_acc: 0.9175\n",
      "Epoch 150/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0338 - acc: 0.9894 - val_loss: 0.3825 - val_acc: 0.9202\n",
      "Epoch 151/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0295 - acc: 0.9918 - val_loss: 0.4578 - val_acc: 0.9072\n",
      "Epoch 152/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0301 - acc: 0.9898 - val_loss: 0.3985 - val_acc: 0.9199\n",
      "Epoch 153/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0306 - acc: 0.9903 - val_loss: 0.3822 - val_acc: 0.9214\n",
      "Epoch 154/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0232 - acc: 0.9916 - val_loss: 0.3978 - val_acc: 0.9187\n",
      "Epoch 155/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0291 - acc: 0.9898 - val_loss: 0.4218 - val_acc: 0.9208\n",
      "Epoch 156/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0257 - acc: 0.9916 - val_loss: 0.4102 - val_acc: 0.9187\n",
      "Epoch 157/600\n",
      "6693/6693 [==============================] - 1s 91us/sample - loss: 0.0286 - acc: 0.9903 - val_loss: 0.4156 - val_acc: 0.9217\n",
      "Epoch 158/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0234 - acc: 0.9927 - val_loss: 0.4076 - val_acc: 0.9184\n",
      "Epoch 159/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0281 - acc: 0.9925 - val_loss: 0.4276 - val_acc: 0.9160\n",
      "Epoch 160/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0403 - acc: 0.9883 - val_loss: 0.3961 - val_acc: 0.9184\n",
      "Epoch 161/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0336 - acc: 0.9904 - val_loss: 0.3830 - val_acc: 0.9239\n",
      "Epoch 162/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0293 - acc: 0.9904 - val_loss: 0.3878 - val_acc: 0.9211\n",
      "Epoch 163/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0229 - acc: 0.9933 - val_loss: 0.4178 - val_acc: 0.9193\n",
      "Epoch 164/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0283 - acc: 0.9910 - val_loss: 0.3932 - val_acc: 0.9230\n",
      "Epoch 165/600\n",
      "6693/6693 [==============================] - 1s 92us/sample - loss: 0.0235 - acc: 0.9927 - val_loss: 0.4068 - val_acc: 0.9217\n",
      "Epoch 166/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0232 - acc: 0.9940 - val_loss: 0.4090 - val_acc: 0.9275\n",
      "Epoch 167/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0206 - acc: 0.9927 - val_loss: 0.4169 - val_acc: 0.9251\n",
      "Epoch 168/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0265 - acc: 0.9919 - val_loss: 0.4300 - val_acc: 0.9190\n",
      "Epoch 169/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0231 - acc: 0.9921 - val_loss: 0.4132 - val_acc: 0.9199\n",
      "Epoch 170/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0237 - acc: 0.9919 - val_loss: 0.3932 - val_acc: 0.9199\n",
      "Epoch 171/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0165 - acc: 0.9945 - val_loss: 0.3851 - val_acc: 0.9214\n",
      "Epoch 172/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0204 - acc: 0.9937 - val_loss: 0.4282 - val_acc: 0.9193\n",
      "Epoch 173/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0167 - acc: 0.9937 - val_loss: 0.4142 - val_acc: 0.9230\n",
      "Epoch 174/600\n",
      "6693/6693 [==============================] - 1s 140us/sample - loss: 0.0231 - acc: 0.9943 - val_loss: 0.4345 - val_acc: 0.9181\n",
      "Epoch 175/600\n",
      "6693/6693 [==============================] - 1s 94us/sample - loss: 0.0252 - acc: 0.9927 - val_loss: 0.4259 - val_acc: 0.9211\n",
      "Epoch 176/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0238 - acc: 0.9916 - val_loss: 0.4146 - val_acc: 0.9217\n",
      "Epoch 177/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0205 - acc: 0.9927 - val_loss: 0.4120 - val_acc: 0.9184\n",
      "Epoch 178/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0289 - acc: 0.9910 - val_loss: 0.4113 - val_acc: 0.9181\n",
      "Epoch 179/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0244 - acc: 0.9927 - val_loss: 0.4055 - val_acc: 0.9196\n",
      "Epoch 180/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0222 - acc: 0.9930 - val_loss: 0.4021 - val_acc: 0.9217\n",
      "Epoch 181/600\n",
      "6693/6693 [==============================] - 1s 92us/sample - loss: 0.0315 - acc: 0.9904 - val_loss: 0.4161 - val_acc: 0.9208\n",
      "Epoch 182/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0270 - acc: 0.9903 - val_loss: 0.3980 - val_acc: 0.9187\n",
      "Epoch 183/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0247 - acc: 0.9921 - val_loss: 0.3811 - val_acc: 0.9227\n",
      "Epoch 184/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0188 - acc: 0.9933 - val_loss: 0.3894 - val_acc: 0.9187\n",
      "Epoch 185/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0136 - acc: 0.9949 - val_loss: 0.4232 - val_acc: 0.9142\n",
      "Epoch 186/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0216 - acc: 0.9945 - val_loss: 0.4181 - val_acc: 0.9120\n",
      "Epoch 187/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0258 - acc: 0.9919 - val_loss: 0.4417 - val_acc: 0.9163\n",
      "Epoch 188/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0242 - acc: 0.9925 - val_loss: 0.4396 - val_acc: 0.9166\n",
      "Epoch 189/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0247 - acc: 0.9927 - val_loss: 0.4208 - val_acc: 0.9172\n",
      "Epoch 190/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0205 - acc: 0.9936 - val_loss: 0.4285 - val_acc: 0.9242\n",
      "Epoch 191/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0243 - acc: 0.9934 - val_loss: 0.4169 - val_acc: 0.9233\n",
      "Epoch 192/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0279 - acc: 0.9928 - val_loss: 0.4016 - val_acc: 0.9239\n",
      "Epoch 193/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0204 - acc: 0.9931 - val_loss: 0.4101 - val_acc: 0.9227\n",
      "Epoch 194/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0206 - acc: 0.9934 - val_loss: 0.3980 - val_acc: 0.9193\n",
      "Epoch 195/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0217 - acc: 0.9937 - val_loss: 0.3776 - val_acc: 0.9214\n",
      "Epoch 196/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0205 - acc: 0.9934 - val_loss: 0.3910 - val_acc: 0.9278\n",
      "Epoch 197/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0213 - acc: 0.9931 - val_loss: 0.4321 - val_acc: 0.9227\n",
      "Epoch 198/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0245 - acc: 0.9913 - val_loss: 0.4292 - val_acc: 0.9230\n",
      "Epoch 199/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0274 - acc: 0.9898 - val_loss: 0.4086 - val_acc: 0.9290\n",
      "Epoch 200/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0180 - acc: 0.9952 - val_loss: 0.4271 - val_acc: 0.9217\n",
      "Epoch 201/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0214 - acc: 0.9924 - val_loss: 0.3905 - val_acc: 0.9263\n",
      "Epoch 202/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0317 - acc: 0.9903 - val_loss: 0.4297 - val_acc: 0.9154\n",
      "Epoch 203/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0261 - acc: 0.9901 - val_loss: 0.4041 - val_acc: 0.9239\n",
      "Epoch 204/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0286 - acc: 0.9915 - val_loss: 0.3940 - val_acc: 0.9224\n",
      "Epoch 205/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0256 - acc: 0.9916 - val_loss: 0.4149 - val_acc: 0.9217\n",
      "Epoch 206/600\n",
      "6693/6693 [==============================] - 1s 92us/sample - loss: 0.0239 - acc: 0.9931 - val_loss: 0.4163 - val_acc: 0.9242\n",
      "Epoch 207/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0194 - acc: 0.9931 - val_loss: 0.3934 - val_acc: 0.9221\n",
      "Epoch 208/600\n",
      "6693/6693 [==============================] - 1s 91us/sample - loss: 0.0138 - acc: 0.9952 - val_loss: 0.4036 - val_acc: 0.9254\n",
      "Epoch 209/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0201 - acc: 0.9934 - val_loss: 0.4056 - val_acc: 0.9236\n",
      "Epoch 210/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0177 - acc: 0.9946 - val_loss: 0.4061 - val_acc: 0.9217\n",
      "Epoch 211/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0200 - acc: 0.9930 - val_loss: 0.3971 - val_acc: 0.9248\n",
      "Epoch 212/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0111 - acc: 0.9957 - val_loss: 0.4255 - val_acc: 0.9236\n",
      "Epoch 213/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0189 - acc: 0.9940 - val_loss: 0.4687 - val_acc: 0.9208\n",
      "Epoch 214/600\n",
      "6693/6693 [==============================] - 1s 94us/sample - loss: 0.0138 - acc: 0.9942 - val_loss: 0.4483 - val_acc: 0.9227\n",
      "Epoch 215/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0193 - acc: 0.9945 - val_loss: 0.4505 - val_acc: 0.9227\n",
      "Epoch 216/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0418 - acc: 0.9870 - val_loss: 0.4119 - val_acc: 0.9178\n",
      "Epoch 217/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0241 - acc: 0.9924 - val_loss: 0.4332 - val_acc: 0.9142\n",
      "Epoch 218/600\n",
      "6693/6693 [==============================] - 1s 93us/sample - loss: 0.0229 - acc: 0.9918 - val_loss: 0.4297 - val_acc: 0.9139\n",
      "Epoch 219/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0182 - acc: 0.9934 - val_loss: 0.4375 - val_acc: 0.9187\n",
      "Epoch 220/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0173 - acc: 0.9940 - val_loss: 0.4220 - val_acc: 0.9196\n",
      "Epoch 221/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0214 - acc: 0.9937 - val_loss: 0.4048 - val_acc: 0.9214\n",
      "Epoch 222/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0177 - acc: 0.9942 - val_loss: 0.4269 - val_acc: 0.9199\n",
      "Epoch 223/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0168 - acc: 0.9948 - val_loss: 0.4353 - val_acc: 0.9227\n",
      "Epoch 224/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0207 - acc: 0.9945 - val_loss: 0.4458 - val_acc: 0.9227\n",
      "Epoch 225/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0201 - acc: 0.9933 - val_loss: 0.4535 - val_acc: 0.9175\n",
      "Epoch 226/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0241 - acc: 0.9921 - val_loss: 0.4566 - val_acc: 0.9211\n",
      "Epoch 227/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0224 - acc: 0.9928 - val_loss: 0.4504 - val_acc: 0.9181\n",
      "Epoch 228/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0237 - acc: 0.9919 - val_loss: 0.4629 - val_acc: 0.9190\n",
      "Epoch 229/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0213 - acc: 0.9937 - val_loss: 0.4391 - val_acc: 0.9178\n",
      "Epoch 230/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0246 - acc: 0.9934 - val_loss: 0.4033 - val_acc: 0.9236\n",
      "Epoch 231/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0120 - acc: 0.9955 - val_loss: 0.4113 - val_acc: 0.9236\n",
      "Epoch 232/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0108 - acc: 0.9969 - val_loss: 0.3890 - val_acc: 0.9269\n",
      "Epoch 233/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0175 - acc: 0.9946 - val_loss: 0.4257 - val_acc: 0.9178\n",
      "Epoch 234/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0200 - acc: 0.9946 - val_loss: 0.4011 - val_acc: 0.9233\n",
      "Epoch 235/600\n",
      "6693/6693 [==============================] - 1s 94us/sample - loss: 0.0206 - acc: 0.9924 - val_loss: 0.4190 - val_acc: 0.9217\n",
      "Epoch 236/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0174 - acc: 0.9948 - val_loss: 0.4532 - val_acc: 0.9184\n",
      "Epoch 237/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0164 - acc: 0.9957 - val_loss: 0.4487 - val_acc: 0.9208\n",
      "Epoch 238/600\n",
      "6693/6693 [==============================] - 1s 116us/sample - loss: 0.0216 - acc: 0.9939 - val_loss: 0.4208 - val_acc: 0.9205\n",
      "Epoch 239/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0235 - acc: 0.9934 - val_loss: 0.3963 - val_acc: 0.9214\n",
      "Epoch 240/600\n",
      "6693/6693 [==============================] - 1s 93us/sample - loss: 0.0191 - acc: 0.9934 - val_loss: 0.4021 - val_acc: 0.9227\n",
      "Epoch 241/600\n",
      "6693/6693 [==============================] - 1s 91us/sample - loss: 0.0116 - acc: 0.9952 - val_loss: 0.4256 - val_acc: 0.9221\n",
      "Epoch 242/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0209 - acc: 0.9937 - val_loss: 0.4385 - val_acc: 0.9214\n",
      "Epoch 243/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0195 - acc: 0.9940 - val_loss: 0.4148 - val_acc: 0.9190\n",
      "Epoch 244/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0253 - acc: 0.9915 - val_loss: 0.4306 - val_acc: 0.9154\n",
      "Epoch 245/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0224 - acc: 0.9928 - val_loss: 0.3784 - val_acc: 0.9230\n",
      "Epoch 246/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0202 - acc: 0.9943 - val_loss: 0.4286 - val_acc: 0.9172\n",
      "Epoch 247/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0179 - acc: 0.9945 - val_loss: 0.3997 - val_acc: 0.9245\n",
      "Epoch 248/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0194 - acc: 0.9934 - val_loss: 0.4151 - val_acc: 0.9196\n",
      "Epoch 249/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0114 - acc: 0.9963 - val_loss: 0.4281 - val_acc: 0.9245\n",
      "Epoch 250/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0188 - acc: 0.9939 - val_loss: 0.4492 - val_acc: 0.9224\n",
      "Epoch 251/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0212 - acc: 0.9934 - val_loss: 0.4344 - val_acc: 0.9196\n",
      "Epoch 252/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0155 - acc: 0.9952 - val_loss: 0.4365 - val_acc: 0.9199\n",
      "Epoch 253/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0135 - acc: 0.9961 - val_loss: 0.4215 - val_acc: 0.9257\n",
      "Epoch 254/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0152 - acc: 0.9946 - val_loss: 0.4351 - val_acc: 0.9248\n",
      "Epoch 255/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0164 - acc: 0.9946 - val_loss: 0.4522 - val_acc: 0.9181\n",
      "Epoch 256/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0205 - acc: 0.9931 - val_loss: 0.4160 - val_acc: 0.9263\n",
      "Epoch 257/600\n",
      "6693/6693 [==============================] - 1s 136us/sample - loss: 0.0248 - acc: 0.9930 - val_loss: 0.4314 - val_acc: 0.9166\n",
      "Epoch 258/600\n",
      "6693/6693 [==============================] - 1s 133us/sample - loss: 0.0194 - acc: 0.9933 - val_loss: 0.4227 - val_acc: 0.9184\n",
      "Epoch 259/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0172 - acc: 0.9949 - val_loss: 0.4736 - val_acc: 0.9239\n",
      "Epoch 260/600\n",
      "6693/6693 [==============================] - 1s 108us/sample - loss: 0.0208 - acc: 0.9940 - val_loss: 0.3963 - val_acc: 0.9230\n",
      "Epoch 261/600\n",
      "6693/6693 [==============================] - 1s 113us/sample - loss: 0.0197 - acc: 0.9939 - val_loss: 0.4447 - val_acc: 0.9184\n",
      "Epoch 262/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0163 - acc: 0.9936 - val_loss: 0.4719 - val_acc: 0.9239\n",
      "Epoch 263/600\n",
      "6693/6693 [==============================] - 1s 92us/sample - loss: 0.0165 - acc: 0.9945 - val_loss: 0.3987 - val_acc: 0.9263\n",
      "Epoch 264/600\n",
      "6693/6693 [==============================] - 1s 94us/sample - loss: 0.0152 - acc: 0.9951 - val_loss: 0.4173 - val_acc: 0.9257\n",
      "Epoch 265/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0164 - acc: 0.9949 - val_loss: 0.4360 - val_acc: 0.9260\n",
      "Epoch 266/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0147 - acc: 0.9945 - val_loss: 0.4287 - val_acc: 0.9281\n",
      "Epoch 267/600\n",
      "6693/6693 [==============================] - 1s 120us/sample - loss: 0.0195 - acc: 0.9937 - val_loss: 0.4695 - val_acc: 0.9184\n",
      "Epoch 268/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0221 - acc: 0.9928 - val_loss: 0.4117 - val_acc: 0.9205\n",
      "Epoch 269/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0174 - acc: 0.9949 - val_loss: 0.4382 - val_acc: 0.9242\n",
      "Epoch 270/600\n",
      "6693/6693 [==============================] - 1s 114us/sample - loss: 0.0166 - acc: 0.9939 - val_loss: 0.4634 - val_acc: 0.9199\n",
      "Epoch 271/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0233 - acc: 0.9930 - val_loss: 0.4775 - val_acc: 0.9163\n",
      "Epoch 272/600\n",
      "6693/6693 [==============================] - 1s 119us/sample - loss: 0.0194 - acc: 0.9940 - val_loss: 0.4373 - val_acc: 0.9227\n",
      "Epoch 273/600\n",
      "6693/6693 [==============================] - 1s 164us/sample - loss: 0.0180 - acc: 0.9937 - val_loss: 0.4504 - val_acc: 0.9211\n",
      "Epoch 274/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0215 - acc: 0.9939 - val_loss: 0.5109 - val_acc: 0.9151\n",
      "Epoch 275/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0151 - acc: 0.9952 - val_loss: 0.4725 - val_acc: 0.9227\n",
      "Epoch 276/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0174 - acc: 0.9936 - val_loss: 0.4947 - val_acc: 0.9160\n",
      "Epoch 277/600\n",
      "6693/6693 [==============================] - 1s 115us/sample - loss: 0.0215 - acc: 0.9933 - val_loss: 0.4892 - val_acc: 0.9163\n",
      "Epoch 278/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0136 - acc: 0.9948 - val_loss: 0.4597 - val_acc: 0.9214\n",
      "Epoch 279/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0169 - acc: 0.9939 - val_loss: 0.4923 - val_acc: 0.9181\n",
      "Epoch 280/600\n",
      "6693/6693 [==============================] - 1s 94us/sample - loss: 0.0148 - acc: 0.9954 - val_loss: 0.4424 - val_acc: 0.9178\n",
      "Epoch 281/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0176 - acc: 0.9928 - val_loss: 0.4790 - val_acc: 0.9163\n",
      "Epoch 282/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0181 - acc: 0.9939 - val_loss: 0.4854 - val_acc: 0.9169\n",
      "Epoch 283/600\n",
      "6693/6693 [==============================] - 1s 91us/sample - loss: 0.0224 - acc: 0.9936 - val_loss: 0.5371 - val_acc: 0.9105\n",
      "Epoch 284/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0290 - acc: 0.9913 - val_loss: 0.4683 - val_acc: 0.9166\n",
      "Epoch 285/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0293 - acc: 0.9907 - val_loss: 0.4543 - val_acc: 0.9126\n",
      "Epoch 286/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0208 - acc: 0.9933 - val_loss: 0.4525 - val_acc: 0.9145\n",
      "Epoch 287/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0169 - acc: 0.9940 - val_loss: 0.4767 - val_acc: 0.9230\n",
      "Epoch 288/600\n",
      "6693/6693 [==============================] - 1s 94us/sample - loss: 0.0167 - acc: 0.9931 - val_loss: 0.4491 - val_acc: 0.9239\n",
      "Epoch 289/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0163 - acc: 0.9945 - val_loss: 0.4457 - val_acc: 0.9199\n",
      "Epoch 290/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0163 - acc: 0.9939 - val_loss: 0.4343 - val_acc: 0.9242\n",
      "Epoch 291/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0130 - acc: 0.9945 - val_loss: 0.4770 - val_acc: 0.9178\n",
      "Epoch 292/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0191 - acc: 0.9933 - val_loss: 0.4256 - val_acc: 0.9208\n",
      "Epoch 293/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0195 - acc: 0.9937 - val_loss: 0.4268 - val_acc: 0.9257\n",
      "Epoch 294/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0202 - acc: 0.9934 - val_loss: 0.4447 - val_acc: 0.9193\n",
      "Epoch 295/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0132 - acc: 0.9964 - val_loss: 0.4333 - val_acc: 0.9214\n",
      "Epoch 296/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0160 - acc: 0.9952 - val_loss: 0.4524 - val_acc: 0.9230\n",
      "Epoch 297/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0110 - acc: 0.9960 - val_loss: 0.4383 - val_acc: 0.9230\n",
      "Epoch 298/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0203 - acc: 0.9933 - val_loss: 0.4766 - val_acc: 0.9193\n",
      "Epoch 299/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0145 - acc: 0.9943 - val_loss: 0.4375 - val_acc: 0.9248\n",
      "Epoch 300/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0162 - acc: 0.9945 - val_loss: 0.4597 - val_acc: 0.9221\n",
      "Epoch 301/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0164 - acc: 0.9958 - val_loss: 0.4709 - val_acc: 0.9190\n",
      "Epoch 302/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0214 - acc: 0.9934 - val_loss: 0.4303 - val_acc: 0.9245\n",
      "Epoch 303/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0149 - acc: 0.9951 - val_loss: 0.4267 - val_acc: 0.9248\n",
      "Epoch 304/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0141 - acc: 0.9948 - val_loss: 0.4418 - val_acc: 0.9230\n",
      "Epoch 305/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0101 - acc: 0.9964 - val_loss: 0.4174 - val_acc: 0.9257\n",
      "Epoch 306/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0144 - acc: 0.9963 - val_loss: 0.4257 - val_acc: 0.9254\n",
      "Epoch 307/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0137 - acc: 0.9961 - val_loss: 0.4665 - val_acc: 0.9205\n",
      "Epoch 308/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0144 - acc: 0.9963 - val_loss: 0.4563 - val_acc: 0.9224\n",
      "Epoch 309/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0151 - acc: 0.9942 - val_loss: 0.4666 - val_acc: 0.9236\n",
      "Epoch 310/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0154 - acc: 0.9945 - val_loss: 0.4951 - val_acc: 0.9172\n",
      "Epoch 311/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0125 - acc: 0.9951 - val_loss: 0.4682 - val_acc: 0.9205\n",
      "Epoch 312/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0166 - acc: 0.9948 - val_loss: 0.4975 - val_acc: 0.9202\n",
      "Epoch 313/600\n",
      "6693/6693 [==============================] - 1s 113us/sample - loss: 0.0158 - acc: 0.9949 - val_loss: 0.4735 - val_acc: 0.9205\n",
      "Epoch 314/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0092 - acc: 0.9963 - val_loss: 0.4826 - val_acc: 0.9254\n",
      "Epoch 315/600\n",
      "6693/6693 [==============================] - 1s 124us/sample - loss: 0.0085 - acc: 0.9976 - val_loss: 0.4806 - val_acc: 0.9230\n",
      "Epoch 316/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0145 - acc: 0.9946 - val_loss: 0.5395 - val_acc: 0.9227\n",
      "Epoch 317/600\n",
      "6693/6693 [==============================] - 1s 113us/sample - loss: 0.0128 - acc: 0.9951 - val_loss: 0.5030 - val_acc: 0.9269\n",
      "Epoch 318/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0156 - acc: 0.9948 - val_loss: 0.4867 - val_acc: 0.9187\n",
      "Epoch 319/600\n",
      "6693/6693 [==============================] - 1s 114us/sample - loss: 0.0203 - acc: 0.9960 - val_loss: 0.4679 - val_acc: 0.9199\n",
      "Epoch 320/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0170 - acc: 0.9942 - val_loss: 0.4701 - val_acc: 0.9217\n",
      "Epoch 321/600\n",
      "6693/6693 [==============================] - 1s 94us/sample - loss: 0.0233 - acc: 0.9936 - val_loss: 0.4933 - val_acc: 0.9251\n",
      "Epoch 322/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0179 - acc: 0.9954 - val_loss: 0.4725 - val_acc: 0.9199\n",
      "Epoch 323/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0155 - acc: 0.9945 - val_loss: 0.4560 - val_acc: 0.9230\n",
      "Epoch 324/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0133 - acc: 0.9946 - val_loss: 0.4728 - val_acc: 0.9184\n",
      "Epoch 325/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0149 - acc: 0.9954 - val_loss: 0.4975 - val_acc: 0.9208\n",
      "Epoch 326/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0213 - acc: 0.9924 - val_loss: 0.4452 - val_acc: 0.9221\n",
      "Epoch 327/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0122 - acc: 0.9963 - val_loss: 0.4836 - val_acc: 0.9217\n",
      "Epoch 328/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0209 - acc: 0.9936 - val_loss: 0.4655 - val_acc: 0.9211\n",
      "Epoch 329/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0154 - acc: 0.9948 - val_loss: 0.4490 - val_acc: 0.9284\n",
      "Epoch 330/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0158 - acc: 0.9942 - val_loss: 0.4370 - val_acc: 0.9272\n",
      "Epoch 331/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0135 - acc: 0.9945 - val_loss: 0.4486 - val_acc: 0.9281\n",
      "Epoch 332/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0138 - acc: 0.9949 - val_loss: 0.4346 - val_acc: 0.9239\n",
      "Epoch 333/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0108 - acc: 0.9961 - val_loss: 0.4549 - val_acc: 0.9248\n",
      "Epoch 334/600\n",
      "6693/6693 [==============================] - 1s 93us/sample - loss: 0.0206 - acc: 0.9949 - val_loss: 0.5240 - val_acc: 0.9211\n",
      "Epoch 335/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0195 - acc: 0.9940 - val_loss: 0.5014 - val_acc: 0.9163\n",
      "Epoch 336/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0329 - acc: 0.9907 - val_loss: 0.4400 - val_acc: 0.9190\n",
      "Epoch 337/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0216 - acc: 0.9933 - val_loss: 0.4881 - val_acc: 0.9178\n",
      "Epoch 338/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0234 - acc: 0.9916 - val_loss: 0.4589 - val_acc: 0.9184\n",
      "Epoch 339/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0168 - acc: 0.9939 - val_loss: 0.5012 - val_acc: 0.9208\n",
      "Epoch 340/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0113 - acc: 0.9958 - val_loss: 0.4771 - val_acc: 0.9233\n",
      "Epoch 341/600\n",
      "6693/6693 [==============================] - 1s 91us/sample - loss: 0.0145 - acc: 0.9949 - val_loss: 0.4934 - val_acc: 0.9163\n",
      "Epoch 342/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0093 - acc: 0.9954 - val_loss: 0.5077 - val_acc: 0.9239\n",
      "Epoch 343/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0130 - acc: 0.9960 - val_loss: 0.5412 - val_acc: 0.9160\n",
      "Epoch 344/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0136 - acc: 0.9952 - val_loss: 0.4697 - val_acc: 0.9227\n",
      "Epoch 345/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0165 - acc: 0.9949 - val_loss: 0.5050 - val_acc: 0.9178\n",
      "Epoch 346/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0148 - acc: 0.9951 - val_loss: 0.4751 - val_acc: 0.9221\n",
      "Epoch 347/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0189 - acc: 0.9945 - val_loss: 0.4703 - val_acc: 0.9196\n",
      "Epoch 348/600\n",
      "6693/6693 [==============================] - 1s 92us/sample - loss: 0.0123 - acc: 0.9961 - val_loss: 0.4740 - val_acc: 0.9214\n",
      "Epoch 349/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0137 - acc: 0.9949 - val_loss: 0.4837 - val_acc: 0.9224\n",
      "Epoch 350/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0141 - acc: 0.9949 - val_loss: 0.4901 - val_acc: 0.9190\n",
      "Epoch 351/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0160 - acc: 0.9948 - val_loss: 0.4480 - val_acc: 0.9187\n",
      "Epoch 352/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0139 - acc: 0.9948 - val_loss: 0.4853 - val_acc: 0.9166\n",
      "Epoch 353/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0133 - acc: 0.9958 - val_loss: 0.4578 - val_acc: 0.9199\n",
      "Epoch 354/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0124 - acc: 0.9955 - val_loss: 0.4720 - val_acc: 0.9202\n",
      "Epoch 355/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0195 - acc: 0.9936 - val_loss: 0.4635 - val_acc: 0.9181\n",
      "Epoch 356/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0129 - acc: 0.9957 - val_loss: 0.4484 - val_acc: 0.9217\n",
      "Epoch 357/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0121 - acc: 0.9957 - val_loss: 0.4616 - val_acc: 0.9190\n",
      "Epoch 358/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0116 - acc: 0.9951 - val_loss: 0.4708 - val_acc: 0.9172\n",
      "Epoch 359/600\n",
      "6693/6693 [==============================] - 1s 91us/sample - loss: 0.0123 - acc: 0.9961 - val_loss: 0.4896 - val_acc: 0.9217\n",
      "Epoch 360/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0108 - acc: 0.9963 - val_loss: 0.4677 - val_acc: 0.9269\n",
      "Epoch 361/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0124 - acc: 0.9961 - val_loss: 0.4851 - val_acc: 0.9251\n",
      "Epoch 362/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0136 - acc: 0.9948 - val_loss: 0.4932 - val_acc: 0.9227\n",
      "Epoch 363/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0164 - acc: 0.9952 - val_loss: 0.4815 - val_acc: 0.9266\n",
      "Epoch 364/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0151 - acc: 0.9946 - val_loss: 0.4896 - val_acc: 0.9208\n",
      "Epoch 365/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0187 - acc: 0.9951 - val_loss: 0.4675 - val_acc: 0.9242\n",
      "Epoch 366/600\n",
      "6693/6693 [==============================] - 1s 92us/sample - loss: 0.0153 - acc: 0.9952 - val_loss: 0.4756 - val_acc: 0.9196\n",
      "Epoch 367/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0199 - acc: 0.9928 - val_loss: 0.5029 - val_acc: 0.9205\n",
      "Epoch 368/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0211 - acc: 0.9937 - val_loss: 0.4545 - val_acc: 0.9224\n",
      "Epoch 369/600\n",
      "6693/6693 [==============================] - 1s 92us/sample - loss: 0.0124 - acc: 0.9951 - val_loss: 0.4725 - val_acc: 0.9224\n",
      "Epoch 370/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0122 - acc: 0.9960 - val_loss: 0.5028 - val_acc: 0.9187\n",
      "Epoch 371/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0142 - acc: 0.9946 - val_loss: 0.5017 - val_acc: 0.9199\n",
      "Epoch 372/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0124 - acc: 0.9957 - val_loss: 0.4868 - val_acc: 0.9224\n",
      "Epoch 373/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0102 - acc: 0.9963 - val_loss: 0.5131 - val_acc: 0.9205\n",
      "Epoch 374/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0168 - acc: 0.9940 - val_loss: 0.4806 - val_acc: 0.9217\n",
      "Epoch 375/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0135 - acc: 0.9958 - val_loss: 0.4823 - val_acc: 0.9224\n",
      "Epoch 376/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0135 - acc: 0.9954 - val_loss: 0.4764 - val_acc: 0.9181\n",
      "Epoch 377/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0192 - acc: 0.9957 - val_loss: 0.5310 - val_acc: 0.9172\n",
      "Epoch 378/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0185 - acc: 0.9943 - val_loss: 0.4773 - val_acc: 0.9193\n",
      "Epoch 379/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0133 - acc: 0.9955 - val_loss: 0.4828 - val_acc: 0.9217\n",
      "Epoch 380/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0150 - acc: 0.9949 - val_loss: 0.4837 - val_acc: 0.9172\n",
      "Epoch 381/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0113 - acc: 0.9966 - val_loss: 0.4907 - val_acc: 0.9202\n",
      "Epoch 382/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0104 - acc: 0.9963 - val_loss: 0.4967 - val_acc: 0.9181\n",
      "Epoch 383/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0114 - acc: 0.9955 - val_loss: 0.4935 - val_acc: 0.9208\n",
      "Epoch 384/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0111 - acc: 0.9958 - val_loss: 0.4875 - val_acc: 0.9205\n",
      "Epoch 385/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0156 - acc: 0.9957 - val_loss: 0.5480 - val_acc: 0.9190\n",
      "Epoch 386/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0143 - acc: 0.9952 - val_loss: 0.5040 - val_acc: 0.9211\n",
      "Epoch 387/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0190 - acc: 0.9952 - val_loss: 0.4803 - val_acc: 0.9257\n",
      "Epoch 388/600\n",
      "6693/6693 [==============================] - 1s 110us/sample - loss: 0.0132 - acc: 0.9958 - val_loss: 0.4548 - val_acc: 0.9305\n",
      "Epoch 389/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0137 - acc: 0.9955 - val_loss: 0.4311 - val_acc: 0.9236\n",
      "Epoch 390/600\n",
      "6693/6693 [==============================] - 1s 128us/sample - loss: 0.0110 - acc: 0.9963 - val_loss: 0.4379 - val_acc: 0.9272\n",
      "Epoch 391/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0155 - acc: 0.9960 - val_loss: 0.4874 - val_acc: 0.9230\n",
      "Epoch 392/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0205 - acc: 0.9937 - val_loss: 0.4972 - val_acc: 0.9217\n",
      "Epoch 393/600\n",
      "6693/6693 [==============================] - 1s 116us/sample - loss: 0.0119 - acc: 0.9948 - val_loss: 0.5184 - val_acc: 0.9254\n",
      "Epoch 394/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0144 - acc: 0.9958 - val_loss: 0.5047 - val_acc: 0.9239\n",
      "Epoch 395/600\n",
      "6693/6693 [==============================] - 1s 94us/sample - loss: 0.0142 - acc: 0.9949 - val_loss: 0.4893 - val_acc: 0.9275\n",
      "Epoch 396/600\n",
      "6693/6693 [==============================] - 1s 93us/sample - loss: 0.0110 - acc: 0.9958 - val_loss: 0.4900 - val_acc: 0.9245\n",
      "Epoch 397/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0157 - acc: 0.9960 - val_loss: 0.5056 - val_acc: 0.9224\n",
      "Epoch 398/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0120 - acc: 0.9960 - val_loss: 0.4600 - val_acc: 0.9221\n",
      "Epoch 399/600\n",
      "6693/6693 [==============================] - 1s 91us/sample - loss: 0.0247 - acc: 0.9930 - val_loss: 0.4627 - val_acc: 0.9266\n",
      "Epoch 400/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0126 - acc: 0.9958 - val_loss: 0.4814 - val_acc: 0.9202\n",
      "Epoch 401/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0126 - acc: 0.9957 - val_loss: 0.4801 - val_acc: 0.9245\n",
      "Epoch 402/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0129 - acc: 0.9952 - val_loss: 0.5013 - val_acc: 0.9245\n",
      "Epoch 403/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0194 - acc: 0.9940 - val_loss: 0.4737 - val_acc: 0.9263\n",
      "Epoch 404/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0141 - acc: 0.9951 - val_loss: 0.4587 - val_acc: 0.9284\n",
      "Epoch 405/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0095 - acc: 0.9960 - val_loss: 0.4551 - val_acc: 0.9302\n",
      "Epoch 406/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0221 - acc: 0.9936 - val_loss: 0.4560 - val_acc: 0.9239\n",
      "Epoch 407/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0158 - acc: 0.9957 - val_loss: 0.4777 - val_acc: 0.9263\n",
      "Epoch 408/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0153 - acc: 0.9964 - val_loss: 0.4809 - val_acc: 0.9266\n",
      "Epoch 409/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0102 - acc: 0.9967 - val_loss: 0.4782 - val_acc: 0.9257\n",
      "Epoch 410/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0086 - acc: 0.9969 - val_loss: 0.4968 - val_acc: 0.9248\n",
      "Epoch 411/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0088 - acc: 0.9966 - val_loss: 0.5054 - val_acc: 0.9263\n",
      "Epoch 412/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0094 - acc: 0.9964 - val_loss: 0.5422 - val_acc: 0.9199\n",
      "Epoch 413/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0113 - acc: 0.9958 - val_loss: 0.4873 - val_acc: 0.9263\n",
      "Epoch 414/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0124 - acc: 0.9952 - val_loss: 0.5156 - val_acc: 0.9236\n",
      "Epoch 415/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0150 - acc: 0.9958 - val_loss: 0.5525 - val_acc: 0.9157\n",
      "Epoch 416/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0124 - acc: 0.9963 - val_loss: 0.5123 - val_acc: 0.9263\n",
      "Epoch 417/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0133 - acc: 0.9954 - val_loss: 0.5090 - val_acc: 0.9272\n",
      "Epoch 418/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0118 - acc: 0.9955 - val_loss: 0.5277 - val_acc: 0.9190\n",
      "Epoch 419/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0101 - acc: 0.9961 - val_loss: 0.5393 - val_acc: 0.9214\n",
      "Epoch 420/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0131 - acc: 0.9964 - val_loss: 0.5138 - val_acc: 0.9217\n",
      "Epoch 421/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0184 - acc: 0.9951 - val_loss: 0.5130 - val_acc: 0.9160\n",
      "Epoch 422/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0164 - acc: 0.9945 - val_loss: 0.5008 - val_acc: 0.9239\n",
      "Epoch 423/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0216 - acc: 0.9939 - val_loss: 0.4639 - val_acc: 0.9227\n",
      "Epoch 424/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0252 - acc: 0.9924 - val_loss: 0.4798 - val_acc: 0.9181\n",
      "Epoch 425/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0142 - acc: 0.9943 - val_loss: 0.4902 - val_acc: 0.9245\n",
      "Epoch 426/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0122 - acc: 0.9958 - val_loss: 0.4425 - val_acc: 0.9221\n",
      "Epoch 427/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0133 - acc: 0.9955 - val_loss: 0.4599 - val_acc: 0.9257\n",
      "Epoch 428/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0085 - acc: 0.9966 - val_loss: 0.4832 - val_acc: 0.9230\n",
      "Epoch 429/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0116 - acc: 0.9960 - val_loss: 0.4852 - val_acc: 0.9263\n",
      "Epoch 430/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0180 - acc: 0.9937 - val_loss: 0.4791 - val_acc: 0.9275\n",
      "Epoch 431/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0128 - acc: 0.9945 - val_loss: 0.4875 - val_acc: 0.9217\n",
      "Epoch 432/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0098 - acc: 0.9960 - val_loss: 0.4801 - val_acc: 0.9266\n",
      "Epoch 433/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0110 - acc: 0.9960 - val_loss: 0.4956 - val_acc: 0.9230\n",
      "Epoch 434/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0144 - acc: 0.9954 - val_loss: 0.4662 - val_acc: 0.9269\n",
      "Epoch 435/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0101 - acc: 0.9966 - val_loss: 0.5029 - val_acc: 0.9248\n",
      "Epoch 436/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0150 - acc: 0.9957 - val_loss: 0.5421 - val_acc: 0.9251\n",
      "Epoch 437/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0134 - acc: 0.9949 - val_loss: 0.5840 - val_acc: 0.9217\n",
      "Epoch 438/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0132 - acc: 0.9958 - val_loss: 0.5592 - val_acc: 0.9184\n",
      "Epoch 439/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0072 - acc: 0.9969 - val_loss: 0.5371 - val_acc: 0.9260\n",
      "Epoch 440/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0106 - acc: 0.9964 - val_loss: 0.5397 - val_acc: 0.9248\n",
      "Epoch 441/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0105 - acc: 0.9960 - val_loss: 0.5405 - val_acc: 0.9239\n",
      "Epoch 442/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0127 - acc: 0.9960 - val_loss: 0.5629 - val_acc: 0.9233\n",
      "Epoch 443/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0111 - acc: 0.9955 - val_loss: 0.5612 - val_acc: 0.9233\n",
      "Epoch 444/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0129 - acc: 0.9952 - val_loss: 0.6037 - val_acc: 0.9202\n",
      "Epoch 445/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0163 - acc: 0.9943 - val_loss: 0.5310 - val_acc: 0.9227\n",
      "Epoch 446/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0111 - acc: 0.9957 - val_loss: 0.5612 - val_acc: 0.9163\n",
      "Epoch 447/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0194 - acc: 0.9955 - val_loss: 0.4780 - val_acc: 0.9199\n",
      "Epoch 448/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0110 - acc: 0.9960 - val_loss: 0.5084 - val_acc: 0.9196\n",
      "Epoch 449/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0096 - acc: 0.9970 - val_loss: 0.5069 - val_acc: 0.9214\n",
      "Epoch 450/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0156 - acc: 0.9951 - val_loss: 0.5035 - val_acc: 0.9208\n",
      "Epoch 451/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0084 - acc: 0.9966 - val_loss: 0.4678 - val_acc: 0.9299\n",
      "Epoch 452/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0081 - acc: 0.9969 - val_loss: 0.5046 - val_acc: 0.9245\n",
      "Epoch 453/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0093 - acc: 0.9969 - val_loss: 0.5152 - val_acc: 0.9257\n",
      "Epoch 454/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0115 - acc: 0.9955 - val_loss: 0.4991 - val_acc: 0.9284\n",
      "Epoch 455/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0087 - acc: 0.9963 - val_loss: 0.5207 - val_acc: 0.9242\n",
      "Epoch 456/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0071 - acc: 0.9975 - val_loss: 0.5609 - val_acc: 0.9245\n",
      "Epoch 457/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0060 - acc: 0.9972 - val_loss: 0.5662 - val_acc: 0.9278\n",
      "Epoch 458/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0145 - acc: 0.9951 - val_loss: 0.5353 - val_acc: 0.9236\n",
      "Epoch 459/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0185 - acc: 0.9955 - val_loss: 0.5347 - val_acc: 0.9230\n",
      "Epoch 460/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0230 - acc: 0.9937 - val_loss: 0.5655 - val_acc: 0.9211\n",
      "Epoch 461/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0138 - acc: 0.9951 - val_loss: 0.4975 - val_acc: 0.9214\n",
      "Epoch 462/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0133 - acc: 0.9960 - val_loss: 0.4998 - val_acc: 0.9181\n",
      "Epoch 463/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0126 - acc: 0.9960 - val_loss: 0.5276 - val_acc: 0.9211\n",
      "Epoch 464/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0240 - acc: 0.9939 - val_loss: 0.4818 - val_acc: 0.9181\n",
      "Epoch 465/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0126 - acc: 0.9955 - val_loss: 0.5145 - val_acc: 0.9208\n",
      "Epoch 466/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0161 - acc: 0.9949 - val_loss: 0.5163 - val_acc: 0.9202\n",
      "Epoch 467/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0095 - acc: 0.9972 - val_loss: 0.5219 - val_acc: 0.9208\n",
      "Epoch 468/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0095 - acc: 0.9963 - val_loss: 0.5143 - val_acc: 0.9242\n",
      "Epoch 469/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0071 - acc: 0.9970 - val_loss: 0.5568 - val_acc: 0.9239\n",
      "Epoch 470/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0083 - acc: 0.9973 - val_loss: 0.5459 - val_acc: 0.9208\n",
      "Epoch 471/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0125 - acc: 0.9954 - val_loss: 0.5478 - val_acc: 0.9233\n",
      "Epoch 472/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0108 - acc: 0.9964 - val_loss: 0.5419 - val_acc: 0.9242\n",
      "Epoch 473/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0132 - acc: 0.9948 - val_loss: 0.5582 - val_acc: 0.9230\n",
      "Epoch 474/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0121 - acc: 0.9954 - val_loss: 0.5514 - val_acc: 0.9227\n",
      "Epoch 475/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0184 - acc: 0.9945 - val_loss: 0.5470 - val_acc: 0.9224\n",
      "Epoch 476/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0100 - acc: 0.9966 - val_loss: 0.5302 - val_acc: 0.9211\n",
      "Epoch 477/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0137 - acc: 0.9949 - val_loss: 0.5555 - val_acc: 0.9199\n",
      "Epoch 478/600\n",
      "6693/6693 [==============================] - 1s 91us/sample - loss: 0.0154 - acc: 0.9946 - val_loss: 0.5691 - val_acc: 0.9172\n",
      "Epoch 479/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0170 - acc: 0.9954 - val_loss: 0.5405 - val_acc: 0.9160\n",
      "Epoch 480/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0139 - acc: 0.9948 - val_loss: 0.5186 - val_acc: 0.9187\n",
      "Epoch 481/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0201 - acc: 0.9949 - val_loss: 0.5277 - val_acc: 0.9187\n",
      "Epoch 482/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0143 - acc: 0.9955 - val_loss: 0.5480 - val_acc: 0.9181\n",
      "Epoch 483/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0124 - acc: 0.9948 - val_loss: 0.5345 - val_acc: 0.9233\n",
      "Epoch 484/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0184 - acc: 0.9948 - val_loss: 0.5146 - val_acc: 0.9202\n",
      "Epoch 485/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0125 - acc: 0.9960 - val_loss: 0.5341 - val_acc: 0.9193\n",
      "Epoch 486/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0196 - acc: 0.9949 - val_loss: 0.5069 - val_acc: 0.9251\n",
      "Epoch 487/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0098 - acc: 0.9957 - val_loss: 0.4960 - val_acc: 0.9236\n",
      "Epoch 488/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0109 - acc: 0.9969 - val_loss: 0.4725 - val_acc: 0.9233\n",
      "Epoch 489/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0119 - acc: 0.9958 - val_loss: 0.5273 - val_acc: 0.9236\n",
      "Epoch 490/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0152 - acc: 0.9960 - val_loss: 0.4456 - val_acc: 0.9315\n",
      "Epoch 491/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0100 - acc: 0.9963 - val_loss: 0.4659 - val_acc: 0.9196\n",
      "Epoch 492/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0099 - acc: 0.9958 - val_loss: 0.4567 - val_acc: 0.9263\n",
      "Epoch 493/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0101 - acc: 0.9963 - val_loss: 0.4582 - val_acc: 0.9257\n",
      "Epoch 494/600\n",
      "6693/6693 [==============================] - 1s 92us/sample - loss: 0.0112 - acc: 0.9969 - val_loss: 0.4813 - val_acc: 0.9254\n",
      "Epoch 495/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0074 - acc: 0.9964 - val_loss: 0.4800 - val_acc: 0.9299\n",
      "Epoch 496/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0110 - acc: 0.9963 - val_loss: 0.5055 - val_acc: 0.9287\n",
      "Epoch 497/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0108 - acc: 0.9973 - val_loss: 0.4727 - val_acc: 0.9308\n",
      "Epoch 498/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0159 - acc: 0.9952 - val_loss: 0.4733 - val_acc: 0.9263\n",
      "Epoch 499/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0133 - acc: 0.9952 - val_loss: 0.4592 - val_acc: 0.9245\n",
      "Epoch 500/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0115 - acc: 0.9961 - val_loss: 0.5180 - val_acc: 0.9214\n",
      "Epoch 501/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0133 - acc: 0.9955 - val_loss: 0.5662 - val_acc: 0.9187\n",
      "Epoch 502/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0140 - acc: 0.9952 - val_loss: 0.5738 - val_acc: 0.9224\n",
      "Epoch 503/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0101 - acc: 0.9958 - val_loss: 0.5566 - val_acc: 0.9275\n",
      "Epoch 504/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0185 - acc: 0.9939 - val_loss: 0.5284 - val_acc: 0.9211\n",
      "Epoch 505/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0127 - acc: 0.9951 - val_loss: 0.4933 - val_acc: 0.9257\n",
      "Epoch 506/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0114 - acc: 0.9963 - val_loss: 0.4934 - val_acc: 0.9272\n",
      "Epoch 507/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0080 - acc: 0.9964 - val_loss: 0.5297 - val_acc: 0.9281\n",
      "Epoch 508/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0161 - acc: 0.9948 - val_loss: 0.5597 - val_acc: 0.9251\n",
      "Epoch 509/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0164 - acc: 0.9958 - val_loss: 0.5056 - val_acc: 0.9284\n",
      "Epoch 510/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0130 - acc: 0.9963 - val_loss: 0.5646 - val_acc: 0.9221\n",
      "Epoch 511/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0132 - acc: 0.9942 - val_loss: 0.5250 - val_acc: 0.9221\n",
      "Epoch 512/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0170 - acc: 0.9937 - val_loss: 0.5285 - val_acc: 0.9181\n",
      "Epoch 513/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0183 - acc: 0.9934 - val_loss: 0.5018 - val_acc: 0.9230\n",
      "Epoch 514/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0107 - acc: 0.9958 - val_loss: 0.5185 - val_acc: 0.9217\n",
      "Epoch 515/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0196 - acc: 0.9957 - val_loss: 0.5104 - val_acc: 0.9257\n",
      "Epoch 516/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0170 - acc: 0.9957 - val_loss: 0.4765 - val_acc: 0.9260\n",
      "Epoch 517/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0111 - acc: 0.9954 - val_loss: 0.4924 - val_acc: 0.9278\n",
      "Epoch 518/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0136 - acc: 0.9963 - val_loss: 0.4879 - val_acc: 0.9272\n",
      "Epoch 519/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0087 - acc: 0.9961 - val_loss: 0.5038 - val_acc: 0.9284\n",
      "Epoch 520/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0126 - acc: 0.9966 - val_loss: 0.5311 - val_acc: 0.9263\n",
      "Epoch 521/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0117 - acc: 0.9963 - val_loss: 0.5447 - val_acc: 0.9254\n",
      "Epoch 522/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0092 - acc: 0.9966 - val_loss: 0.5698 - val_acc: 0.9260\n",
      "Epoch 523/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0111 - acc: 0.9972 - val_loss: 0.5500 - val_acc: 0.9281\n",
      "Epoch 524/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0107 - acc: 0.9952 - val_loss: 0.5242 - val_acc: 0.9254\n",
      "Epoch 525/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0129 - acc: 0.9957 - val_loss: 0.5790 - val_acc: 0.9208\n",
      "Epoch 526/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0197 - acc: 0.9940 - val_loss: 0.4960 - val_acc: 0.9263\n",
      "Epoch 527/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0136 - acc: 0.9954 - val_loss: 0.4987 - val_acc: 0.9248\n",
      "Epoch 528/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0108 - acc: 0.9960 - val_loss: 0.4911 - val_acc: 0.9293\n",
      "Epoch 529/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0097 - acc: 0.9958 - val_loss: 0.4968 - val_acc: 0.9290\n",
      "Epoch 530/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0149 - acc: 0.9952 - val_loss: 0.5123 - val_acc: 0.9251\n",
      "Epoch 531/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0164 - acc: 0.9963 - val_loss: 0.4996 - val_acc: 0.9263\n",
      "Epoch 532/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0098 - acc: 0.9966 - val_loss: 0.5258 - val_acc: 0.9275\n",
      "Epoch 533/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0075 - acc: 0.9970 - val_loss: 0.5417 - val_acc: 0.9275\n",
      "Epoch 534/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0077 - acc: 0.9964 - val_loss: 0.5203 - val_acc: 0.9293\n",
      "Epoch 535/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0066 - acc: 0.9966 - val_loss: 0.5613 - val_acc: 0.9248\n",
      "Epoch 536/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0073 - acc: 0.9976 - val_loss: 0.5716 - val_acc: 0.9242\n",
      "Epoch 537/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0144 - acc: 0.9960 - val_loss: 0.5816 - val_acc: 0.9272\n",
      "Epoch 538/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0212 - acc: 0.9946 - val_loss: 0.5310 - val_acc: 0.9278\n",
      "Epoch 539/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.5475 - val_acc: 0.9227\n",
      "Epoch 540/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0074 - acc: 0.9973 - val_loss: 0.5163 - val_acc: 0.9266\n",
      "Epoch 541/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0084 - acc: 0.9960 - val_loss: 0.5128 - val_acc: 0.9266\n",
      "Epoch 542/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0138 - acc: 0.9952 - val_loss: 0.5499 - val_acc: 0.9290\n",
      "Epoch 543/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0157 - acc: 0.9958 - val_loss: 0.5614 - val_acc: 0.9233\n",
      "Epoch 544/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0178 - acc: 0.9943 - val_loss: 0.4842 - val_acc: 0.9290\n",
      "Epoch 545/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0131 - acc: 0.9952 - val_loss: 0.5217 - val_acc: 0.9214\n",
      "Epoch 546/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0080 - acc: 0.9964 - val_loss: 0.5248 - val_acc: 0.9230\n",
      "Epoch 547/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0090 - acc: 0.9967 - val_loss: 0.5273 - val_acc: 0.9217\n",
      "Epoch 548/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0088 - acc: 0.9970 - val_loss: 0.5892 - val_acc: 0.9172\n",
      "Epoch 549/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0169 - acc: 0.9951 - val_loss: 0.5487 - val_acc: 0.9230\n",
      "Epoch 550/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0156 - acc: 0.9949 - val_loss: 0.5259 - val_acc: 0.9245\n",
      "Epoch 551/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0094 - acc: 0.9972 - val_loss: 0.5103 - val_acc: 0.9281\n",
      "Epoch 552/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0126 - acc: 0.9964 - val_loss: 0.5220 - val_acc: 0.9266\n",
      "Epoch 553/600\n",
      "6693/6693 [==============================] - 1s 92us/sample - loss: 0.0076 - acc: 0.9966 - val_loss: 0.5419 - val_acc: 0.9236\n",
      "Epoch 554/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0077 - acc: 0.9975 - val_loss: 0.5336 - val_acc: 0.9266\n",
      "Epoch 555/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0073 - acc: 0.9976 - val_loss: 0.5537 - val_acc: 0.9236\n",
      "Epoch 556/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0122 - acc: 0.9958 - val_loss: 0.5121 - val_acc: 0.9266\n",
      "Epoch 557/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0117 - acc: 0.9967 - val_loss: 0.5303 - val_acc: 0.9239\n",
      "Epoch 558/600\n",
      "6693/6693 [==============================] - 1s 91us/sample - loss: 0.0079 - acc: 0.9969 - val_loss: 0.5441 - val_acc: 0.9239\n",
      "Epoch 559/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0134 - acc: 0.9958 - val_loss: 0.5284 - val_acc: 0.9233\n",
      "Epoch 560/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0122 - acc: 0.9969 - val_loss: 0.5965 - val_acc: 0.9214\n",
      "Epoch 561/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0137 - acc: 0.9961 - val_loss: 0.5565 - val_acc: 0.9214\n",
      "Epoch 562/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0128 - acc: 0.9952 - val_loss: 0.5332 - val_acc: 0.9217\n",
      "Epoch 563/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0195 - acc: 0.9936 - val_loss: 0.6178 - val_acc: 0.9169\n",
      "Epoch 564/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0085 - acc: 0.9964 - val_loss: 0.5734 - val_acc: 0.9205\n",
      "Epoch 565/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0137 - acc: 0.9957 - val_loss: 0.5761 - val_acc: 0.9211\n",
      "Epoch 566/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0096 - acc: 0.9963 - val_loss: 0.5703 - val_acc: 0.9230\n",
      "Epoch 567/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0095 - acc: 0.9963 - val_loss: 0.5682 - val_acc: 0.9202\n",
      "Epoch 568/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0116 - acc: 0.9957 - val_loss: 0.5458 - val_acc: 0.9224\n",
      "Epoch 569/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0070 - acc: 0.9976 - val_loss: 0.5620 - val_acc: 0.9245\n",
      "Epoch 570/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0210 - acc: 0.9948 - val_loss: 0.5413 - val_acc: 0.9199\n",
      "Epoch 571/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 0.0142 - acc: 0.9957 - val_loss: 0.5282 - val_acc: 0.9196\n",
      "Epoch 572/600\n",
      "6693/6693 [==============================] - 1s 93us/sample - loss: 0.0169 - acc: 0.9949 - val_loss: 0.5047 - val_acc: 0.9245\n",
      "Epoch 573/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0125 - acc: 0.9955 - val_loss: 0.5158 - val_acc: 0.9190\n",
      "Epoch 574/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0145 - acc: 0.9952 - val_loss: 0.5071 - val_acc: 0.9248\n",
      "Epoch 575/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0177 - acc: 0.9952 - val_loss: 0.5037 - val_acc: 0.9205\n",
      "Epoch 576/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0091 - acc: 0.9960 - val_loss: 0.4999 - val_acc: 0.9217\n",
      "Epoch 577/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0100 - acc: 0.9967 - val_loss: 0.5111 - val_acc: 0.9239\n",
      "Epoch 578/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0091 - acc: 0.9963 - val_loss: 0.5278 - val_acc: 0.9193\n",
      "Epoch 579/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0115 - acc: 0.9963 - val_loss: 0.5448 - val_acc: 0.9227\n",
      "Epoch 580/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0074 - acc: 0.9969 - val_loss: 0.5493 - val_acc: 0.9214\n",
      "Epoch 581/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0080 - acc: 0.9970 - val_loss: 0.5804 - val_acc: 0.9205\n",
      "Epoch 582/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0096 - acc: 0.9967 - val_loss: 0.5444 - val_acc: 0.9254\n",
      "Epoch 583/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0144 - acc: 0.9958 - val_loss: 0.5235 - val_acc: 0.9221\n",
      "Epoch 584/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0108 - acc: 0.9964 - val_loss: 0.5136 - val_acc: 0.9242\n",
      "Epoch 585/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0107 - acc: 0.9961 - val_loss: 0.5709 - val_acc: 0.9217\n",
      "Epoch 586/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0240 - acc: 0.9934 - val_loss: 0.5311 - val_acc: 0.9178\n",
      "Epoch 587/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0122 - acc: 0.9954 - val_loss: 0.5009 - val_acc: 0.9227\n",
      "Epoch 588/600\n",
      "6693/6693 [==============================] - 1s 88us/sample - loss: 0.0173 - acc: 0.9945 - val_loss: 0.5376 - val_acc: 0.9181\n",
      "Epoch 589/600\n",
      "6693/6693 [==============================] - 1s 89us/sample - loss: 0.0135 - acc: 0.9958 - val_loss: 0.5249 - val_acc: 0.9205\n",
      "Epoch 590/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0085 - acc: 0.9970 - val_loss: 0.5656 - val_acc: 0.9193\n",
      "Epoch 591/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0108 - acc: 0.9969 - val_loss: 0.5042 - val_acc: 0.9211\n",
      "Epoch 592/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0070 - acc: 0.9969 - val_loss: 0.5417 - val_acc: 0.9208\n",
      "Epoch 593/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0141 - acc: 0.9961 - val_loss: 0.5788 - val_acc: 0.9193\n",
      "Epoch 594/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0163 - acc: 0.9943 - val_loss: 0.5606 - val_acc: 0.9211\n",
      "Epoch 595/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0135 - acc: 0.9964 - val_loss: 0.6086 - val_acc: 0.9175\n",
      "Epoch 596/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0248 - acc: 0.9945 - val_loss: 0.5615 - val_acc: 0.9178\n",
      "Epoch 597/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0127 - acc: 0.9955 - val_loss: 0.5799 - val_acc: 0.9208\n",
      "Epoch 598/600\n",
      "6693/6693 [==============================] - 1s 86us/sample - loss: 0.0094 - acc: 0.9966 - val_loss: 0.5364 - val_acc: 0.9224\n",
      "Epoch 599/600\n",
      "6693/6693 [==============================] - 1s 85us/sample - loss: 0.0103 - acc: 0.9964 - val_loss: 0.5283 - val_acc: 0.9217\n",
      "Epoch 600/600\n",
      "6693/6693 [==============================] - 1s 87us/sample - loss: 0.0109 - acc: 0.9970 - val_loss: 0.4977 - val_acc: 0.9245\n"
     ]
    }
   ],
   "source": [
    "model_history = trainModel(model=model, epochs=600, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3297/3297 [==============================] - 0s 23us/sample - loss: 0.4977 - acc: 0.9245\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss is:  0.4977346794656598\n",
      "The Best test Accuracy is:  92.44768023490906\n"
     ]
    }
   ],
   "source": [
    "print('The test loss is: ', test_loss)\n",
    "print('The Best test Accuracy is: ', test_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1677081be88>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorylist = list(df.select_dtypes(['object']))\n",
    "tempdf = pd.get_dummies(df, columns=categorylist)\n",
    "label_columns = [x for x in tempdf.columns if x.startswith('label')]\n",
    "X = tempdf.drop(label_columns, axis=1)\n",
    "y = tempdf[label_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Sam\\miniconda3\\envs\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "categorylist = list(df.select_dtypes(['object']))\n",
    "y = df[categorylist]\n",
    "y_encoded = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_multilabel_to_onehot(array, column='label', return_columns=False):\n",
    "    df = pd.DataFrame(y_test, columns=[column])\n",
    "    df = pd.get_dummies(df, columns=[column])\n",
    "    \n",
    "    if return_columns:\n",
    "        return [x.replace('label_','') for x in df.columns]\n",
    "    return df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list(X.select_dtypes(['int64', 'float64']))\n",
    "pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prepared = pipeline.fit_transform(X)\n",
    "# y_prepared = y.to_numpy()[:,1]\n",
    "y_prepared = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=10, test_size= 0.33, random_state=0)\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    ExtraTreeClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "labels = [x.replace('label_','') for x in y.columns]\n",
    "for train_idx, test_idx in sss.split(X_prepared, y_prepared):\n",
    "    X_train, X_test = X_prepared[train_idx], X_prepared[test_idx]\n",
    "    y_train, y_test = y_prepared[train_idx], y_prepared[test_idx]\n",
    "    for clf in classifiers:\n",
    "        name = clf.__class__.__name__\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred_probas = clf.predict_proba(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        loss = log_loss(y_test, y_pred)\n",
    "        \n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(y_test[:,i], y_pred_probas[i][:,1])\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "            res.append([name, label, acc, loss, roc_auc[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "res_svc = []\n",
    "clf = SVC(probability=True)\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(sss.split(X_prepared, y_encoded)):\n",
    "    print(fold_idx)\n",
    "    X_train, X_test = X_prepared[train_idx], X_prepared[test_idx]\n",
    "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "    name = clf.__class__.__name__\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_probas = clf.predict_proba(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    loss = log_loss(y_test, y_pred_probas)\n",
    "    \n",
    "    labels = from_multilabel_to_onehot(y_test, return_columns=True)\n",
    "    y_test = from_multilabel_to_onehot(y_test)\n",
    "    # y_pred_probas = from_multilabel_to_onehot(y_pred_probas)\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "    for i, label in enumerate(labels):\n",
    "        fpr[label], tpr[label], _ = roc_curve(y_test[:,i], y_pred_probas[:,i])\n",
    "        roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "        res_svc.append([name, label, acc, loss, roc_auc[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.848286</td>\n",
       "      <td>0.457025</td>\n",
       "      <td>0.986206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.833910</td>\n",
       "      <td>2.898170</td>\n",
       "      <td>0.981071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.627237</td>\n",
       "      <td>12.851316</td>\n",
       "      <td>0.793185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>0.534577</td>\n",
       "      <td>16.056568</td>\n",
       "      <td>0.741620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Classifier  Accuracy   Log Loss       AUC\n",
       "0                     SVC  0.848286   0.457025  0.986206\n",
       "2    KNeighborsClassifier  0.833910   2.898170  0.981071\n",
       "0  DecisionTreeClassifier  0.627237  12.851316  0.793185\n",
       "1     ExtraTreeClassifier  0.534577  16.056568  0.741620"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_svc = pd.DataFrame(res_svc, columns=['Classifier','Label', 'Accuracy','Log Loss','AUC'])\n",
    "log = pd.DataFrame(res, columns=['Classifier','Label', 'Accuracy','Log Loss','AUC'])\n",
    "a = log_svc.groupby(['Classifier'], as_index=False).mean()\n",
    "b = log.groupby(['Classifier'], as_index=False).mean()\n",
    "pd.concat([b,a],axis=0).sort_values(by='Accuracy', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1770bc253cc25a4f6117d11136c6fe1ae98e60103fb41d13511f1749add39a66"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import scipy\r\n",
    "import sys, os, pickle\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "\r\n",
    "\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "# from sklearn.preprocessing import OneHotEncoder\r\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "\r\n",
    "#classifier models\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.tree import ExtraTreeClassifier\r\n",
    "from sklearn.svm import SVC\r\n",
    "\r\n",
    "\r\n",
    "#metrics\r\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_curve, auc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "df = pd.read_csv(r'./data/features_3_sec.csv')\r\n",
    "df = df.drop(labels='filename', axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "y = LabelEncoder().fit_transform(df.iloc[:,-1])\r\n",
    "X = StandardScaler().fit_transform(np.array(df.iloc[:,:-1], dtype=float))\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Learning model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def trainModel(model, epochs, optimizer):\r\n",
    "    batch_size = 128\r\n",
    "    #callback = myCallback()\r\n",
    "    model.compile(optimizer=optimizer,\r\n",
    "                  loss=\"sparse_categorical_crossentropy\",\r\n",
    "                  metrics=['accuracy'])\r\n",
    "    return model.fit(X_train, y_train, validation_data=(X_test, y_test),\r\n",
    "                     epochs=epochs, batch_size=batch_size)\r\n",
    "\r\n",
    "def plotValidate(history):\r\n",
    "    print(\"Validation Accuracy\", max(history.history['val_accuracy']))\r\n",
    "    pd.DataFrame(history.history).plot(figsize=(12,6))\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "units, rate = 64, 0.2\r\n",
    "model = keras.models.Sequential([\r\n",
    "    keras.layers.Dense(units*2**3, activation='relu', input_shape=(X_train.shape[1],)),\r\n",
    "    keras.layers.Dropout(rate),\r\n",
    "    \r\n",
    "    keras.layers.Dense(units*2**2, activation='relu'),\r\n",
    "    keras.layers.Dropout(rate),\r\n",
    "    \r\n",
    "    keras.layers.Dense(units*2**1, activation='relu'),\r\n",
    "    keras.layers.Dropout(rate),\r\n",
    "    \r\n",
    "    keras.layers.Dense(units, activation='relu'),\r\n",
    "    keras.layers.Dropout(rate),\r\n",
    "    \r\n",
    "    keras.layers.Dense(10, activation='softmax'),\r\n",
    "])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\Sam\\miniconda3\\envs\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'AdaBoostClassifier' object has no attribute 'summary'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'AdaBoostClassifier' object has no attribute 'summary'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_history = trainModel(model=model, epochs=600, optimizer='adam')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 6693 samples, validate on 3297 samples\n",
      "Epoch 1/600\n",
      "6693/6693 [==============================] - 2s 347us/sample - loss: 1.7018 - acc: 0.4009 - val_loss: 1.1784 - val_acc: 0.5866\n",
      "Epoch 2/600\n",
      "6693/6693 [==============================] - 1s 90us/sample - loss: 1.1701 - acc: 0.5933 - val_loss: 0.8748 - val_acc: 0.7040\n",
      "Epoch 3/600\n",
      "6693/6693 [==============================] - 1s 79us/sample - loss: 0.9305 - acc: 0.6834 - val_loss: 0.7721 - val_acc: 0.7398\n",
      "Epoch 4/600\n",
      "6693/6693 [==============================] - 1s 78us/sample - loss: 0.8026 - acc: 0.7282 - val_loss: 0.7081 - val_acc: 0.7513\n",
      "Epoch 5/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.7138 - acc: 0.7606 - val_loss: 0.6387 - val_acc: 0.7904\n",
      "Epoch 6/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.6324 - acc: 0.7880 - val_loss: 0.5700 - val_acc: 0.8150\n",
      "Epoch 7/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.5869 - acc: 0.8050 - val_loss: 0.5676 - val_acc: 0.8113\n",
      "Epoch 8/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.5234 - acc: 0.8238 - val_loss: 0.5076 - val_acc: 0.8320\n",
      "Epoch 9/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.4801 - acc: 0.8373 - val_loss: 0.4795 - val_acc: 0.8371\n",
      "Epoch 10/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.4454 - acc: 0.8481 - val_loss: 0.4436 - val_acc: 0.8599\n",
      "Epoch 11/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.4000 - acc: 0.8703 - val_loss: 0.4531 - val_acc: 0.8541\n",
      "Epoch 12/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.3666 - acc: 0.8772 - val_loss: 0.4495 - val_acc: 0.8529\n",
      "Epoch 13/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.3342 - acc: 0.8863 - val_loss: 0.4098 - val_acc: 0.8702\n",
      "Epoch 14/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.3096 - acc: 0.8959 - val_loss: 0.4300 - val_acc: 0.8711\n",
      "Epoch 15/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.2889 - acc: 0.9044 - val_loss: 0.3790 - val_acc: 0.8799\n",
      "Epoch 16/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.2786 - acc: 0.9074 - val_loss: 0.3931 - val_acc: 0.8802\n",
      "Epoch 17/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.2382 - acc: 0.9196 - val_loss: 0.3613 - val_acc: 0.8908\n",
      "Epoch 18/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.2231 - acc: 0.9251 - val_loss: 0.3911 - val_acc: 0.8854\n",
      "Epoch 19/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.2200 - acc: 0.9274 - val_loss: 0.3580 - val_acc: 0.8923\n",
      "Epoch 20/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.2065 - acc: 0.9350 - val_loss: 0.3558 - val_acc: 0.8932\n",
      "Epoch 21/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.1855 - acc: 0.9408 - val_loss: 0.3891 - val_acc: 0.8890\n",
      "Epoch 22/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.1788 - acc: 0.9402 - val_loss: 0.3520 - val_acc: 0.8975\n",
      "Epoch 23/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.1544 - acc: 0.9514 - val_loss: 0.3612 - val_acc: 0.9005\n",
      "Epoch 24/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.1548 - acc: 0.9468 - val_loss: 0.3632 - val_acc: 0.8960\n",
      "Epoch 25/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.1286 - acc: 0.9573 - val_loss: 0.3738 - val_acc: 0.8960\n",
      "Epoch 26/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.1362 - acc: 0.9556 - val_loss: 0.3662 - val_acc: 0.9042\n",
      "Epoch 27/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.1488 - acc: 0.9489 - val_loss: 0.3708 - val_acc: 0.8926\n",
      "Epoch 28/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.1301 - acc: 0.9571 - val_loss: 0.3435 - val_acc: 0.9060\n",
      "Epoch 29/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.1256 - acc: 0.9638 - val_loss: 0.3643 - val_acc: 0.9045\n",
      "Epoch 30/600\n",
      "6693/6693 [==============================] - 1s 110us/sample - loss: 0.1135 - acc: 0.9610 - val_loss: 0.3776 - val_acc: 0.8990\n",
      "Epoch 31/600\n",
      "6693/6693 [==============================] - 1s 121us/sample - loss: 0.1132 - acc: 0.9652 - val_loss: 0.3718 - val_acc: 0.9032\n",
      "Epoch 32/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.1060 - acc: 0.9644 - val_loss: 0.3322 - val_acc: 0.9066\n",
      "Epoch 33/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0958 - acc: 0.9692 - val_loss: 0.3596 - val_acc: 0.9051\n",
      "Epoch 34/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0935 - acc: 0.9679 - val_loss: 0.3694 - val_acc: 0.9081\n",
      "Epoch 35/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0971 - acc: 0.9700 - val_loss: 0.3920 - val_acc: 0.9029\n",
      "Epoch 36/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0890 - acc: 0.9710 - val_loss: 0.3838 - val_acc: 0.9048\n",
      "Epoch 37/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0803 - acc: 0.9739 - val_loss: 0.4206 - val_acc: 0.9008\n",
      "Epoch 38/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0947 - acc: 0.9695 - val_loss: 0.3856 - val_acc: 0.9014\n",
      "Epoch 39/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0844 - acc: 0.9713 - val_loss: 0.3840 - val_acc: 0.9017\n",
      "Epoch 40/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0804 - acc: 0.9737 - val_loss: 0.3821 - val_acc: 0.9032\n",
      "Epoch 41/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0908 - acc: 0.9737 - val_loss: 0.3656 - val_acc: 0.9039\n",
      "Epoch 42/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0720 - acc: 0.9780 - val_loss: 0.3785 - val_acc: 0.9066\n",
      "Epoch 43/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0823 - acc: 0.9752 - val_loss: 0.3856 - val_acc: 0.9057\n",
      "Epoch 44/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0748 - acc: 0.9734 - val_loss: 0.4060 - val_acc: 0.9075\n",
      "Epoch 45/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0739 - acc: 0.9762 - val_loss: 0.3614 - val_acc: 0.9093\n",
      "Epoch 46/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0695 - acc: 0.9770 - val_loss: 0.3447 - val_acc: 0.9145\n",
      "Epoch 47/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0672 - acc: 0.9807 - val_loss: 0.3681 - val_acc: 0.9026\n",
      "Epoch 48/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0588 - acc: 0.9780 - val_loss: 0.3725 - val_acc: 0.9099\n",
      "Epoch 49/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0713 - acc: 0.9785 - val_loss: 0.3730 - val_acc: 0.9060\n",
      "Epoch 50/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0586 - acc: 0.9803 - val_loss: 0.3976 - val_acc: 0.9072\n",
      "Epoch 51/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0642 - acc: 0.9782 - val_loss: 0.3879 - val_acc: 0.9054\n",
      "Epoch 52/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0643 - acc: 0.9803 - val_loss: 0.3696 - val_acc: 0.9111\n",
      "Epoch 53/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0551 - acc: 0.9828 - val_loss: 0.3964 - val_acc: 0.9090\n",
      "Epoch 54/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0630 - acc: 0.9794 - val_loss: 0.4032 - val_acc: 0.9072\n",
      "Epoch 55/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0741 - acc: 0.9776 - val_loss: 0.3680 - val_acc: 0.9090\n",
      "Epoch 56/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0575 - acc: 0.9815 - val_loss: 0.3878 - val_acc: 0.9136\n",
      "Epoch 57/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0557 - acc: 0.9810 - val_loss: 0.3885 - val_acc: 0.9084\n",
      "Epoch 58/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0518 - acc: 0.9849 - val_loss: 0.3584 - val_acc: 0.9136\n",
      "Epoch 59/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0488 - acc: 0.9842 - val_loss: 0.3785 - val_acc: 0.9154\n",
      "Epoch 60/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0523 - acc: 0.9834 - val_loss: 0.3582 - val_acc: 0.9154\n",
      "Epoch 61/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0478 - acc: 0.9863 - val_loss: 0.3454 - val_acc: 0.9175\n",
      "Epoch 62/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0435 - acc: 0.9872 - val_loss: 0.3853 - val_acc: 0.9145\n",
      "Epoch 63/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0521 - acc: 0.9845 - val_loss: 0.3962 - val_acc: 0.9136\n",
      "Epoch 64/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0448 - acc: 0.9855 - val_loss: 0.4290 - val_acc: 0.9114\n",
      "Epoch 65/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0480 - acc: 0.9861 - val_loss: 0.4037 - val_acc: 0.9102\n",
      "Epoch 66/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0507 - acc: 0.9837 - val_loss: 0.3900 - val_acc: 0.9075\n",
      "Epoch 67/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0486 - acc: 0.9843 - val_loss: 0.3927 - val_acc: 0.9060\n",
      "Epoch 68/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0551 - acc: 0.9824 - val_loss: 0.4435 - val_acc: 0.8990\n",
      "Epoch 69/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0446 - acc: 0.9843 - val_loss: 0.4212 - val_acc: 0.9120\n",
      "Epoch 70/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0501 - acc: 0.9845 - val_loss: 0.4381 - val_acc: 0.9081\n",
      "Epoch 71/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0528 - acc: 0.9837 - val_loss: 0.4299 - val_acc: 0.9045\n",
      "Epoch 72/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0529 - acc: 0.9846 - val_loss: 0.4106 - val_acc: 0.9066\n",
      "Epoch 73/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0447 - acc: 0.9858 - val_loss: 0.4206 - val_acc: 0.9075\n",
      "Epoch 74/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0388 - acc: 0.9872 - val_loss: 0.4183 - val_acc: 0.9120\n",
      "Epoch 75/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0381 - acc: 0.9861 - val_loss: 0.4358 - val_acc: 0.9096\n",
      "Epoch 76/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0453 - acc: 0.9869 - val_loss: 0.4469 - val_acc: 0.9054\n",
      "Epoch 77/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0560 - acc: 0.9828 - val_loss: 0.4336 - val_acc: 0.9032\n",
      "Epoch 78/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0427 - acc: 0.9858 - val_loss: 0.4121 - val_acc: 0.9145\n",
      "Epoch 79/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0389 - acc: 0.9869 - val_loss: 0.3930 - val_acc: 0.9160\n",
      "Epoch 80/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0443 - acc: 0.9863 - val_loss: 0.3860 - val_acc: 0.9157\n",
      "Epoch 81/600\n",
      "6693/6693 [==============================] - 1s 110us/sample - loss: 0.0322 - acc: 0.9906 - val_loss: 0.4011 - val_acc: 0.9190\n",
      "Epoch 82/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0356 - acc: 0.9894 - val_loss: 0.4149 - val_acc: 0.9145\n",
      "Epoch 83/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0530 - acc: 0.9848 - val_loss: 0.3979 - val_acc: 0.9157\n",
      "Epoch 84/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0485 - acc: 0.9822 - val_loss: 0.4263 - val_acc: 0.9093\n",
      "Epoch 85/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0431 - acc: 0.9848 - val_loss: 0.4071 - val_acc: 0.9120\n",
      "Epoch 86/600\n",
      "6693/6693 [==============================] - 1s 112us/sample - loss: 0.0474 - acc: 0.9837 - val_loss: 0.4037 - val_acc: 0.9123\n",
      "Epoch 87/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0412 - acc: 0.9877 - val_loss: 0.3959 - val_acc: 0.9163\n",
      "Epoch 88/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0297 - acc: 0.9904 - val_loss: 0.3831 - val_acc: 0.9175\n",
      "Epoch 89/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0336 - acc: 0.9910 - val_loss: 0.4126 - val_acc: 0.9126\n",
      "Epoch 90/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0368 - acc: 0.9877 - val_loss: 0.3893 - val_acc: 0.9208\n",
      "Epoch 91/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0472 - acc: 0.9879 - val_loss: 0.3894 - val_acc: 0.9163\n",
      "Epoch 92/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0343 - acc: 0.9889 - val_loss: 0.3754 - val_acc: 0.9148\n",
      "Epoch 93/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0408 - acc: 0.9888 - val_loss: 0.4053 - val_acc: 0.9123\n",
      "Epoch 94/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0395 - acc: 0.9872 - val_loss: 0.4151 - val_acc: 0.9136\n",
      "Epoch 95/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0422 - acc: 0.9860 - val_loss: 0.3489 - val_acc: 0.9175\n",
      "Epoch 96/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0274 - acc: 0.9910 - val_loss: 0.3902 - val_acc: 0.9166\n",
      "Epoch 97/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0373 - acc: 0.9891 - val_loss: 0.3849 - val_acc: 0.9084\n",
      "Epoch 98/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0318 - acc: 0.9889 - val_loss: 0.4111 - val_acc: 0.9133\n",
      "Epoch 99/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0372 - acc: 0.9869 - val_loss: 0.4245 - val_acc: 0.9123\n",
      "Epoch 100/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0390 - acc: 0.9873 - val_loss: 0.4197 - val_acc: 0.9102\n",
      "Epoch 101/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0338 - acc: 0.9889 - val_loss: 0.4175 - val_acc: 0.9136\n",
      "Epoch 102/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0326 - acc: 0.9885 - val_loss: 0.4350 - val_acc: 0.9117\n",
      "Epoch 103/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0440 - acc: 0.9849 - val_loss: 0.3916 - val_acc: 0.9184\n",
      "Epoch 104/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0418 - acc: 0.9880 - val_loss: 0.4282 - val_acc: 0.9114\n",
      "Epoch 105/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0429 - acc: 0.9873 - val_loss: 0.4415 - val_acc: 0.9093\n",
      "Epoch 106/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0453 - acc: 0.9879 - val_loss: 0.4682 - val_acc: 0.9032\n",
      "Epoch 107/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0486 - acc: 0.9867 - val_loss: 0.4351 - val_acc: 0.9090\n",
      "Epoch 108/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0441 - acc: 0.9863 - val_loss: 0.4289 - val_acc: 0.9139\n",
      "Epoch 109/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0363 - acc: 0.9889 - val_loss: 0.3984 - val_acc: 0.9166\n",
      "Epoch 110/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0378 - acc: 0.9867 - val_loss: 0.4183 - val_acc: 0.9093\n",
      "Epoch 111/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0376 - acc: 0.9883 - val_loss: 0.3834 - val_acc: 0.9142\n",
      "Epoch 112/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0291 - acc: 0.9889 - val_loss: 0.4201 - val_acc: 0.9136\n",
      "Epoch 113/600\n",
      "6693/6693 [==============================] - 1s 120us/sample - loss: 0.0262 - acc: 0.9909 - val_loss: 0.3869 - val_acc: 0.9175\n",
      "Epoch 114/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0336 - acc: 0.9889 - val_loss: 0.3822 - val_acc: 0.9196\n",
      "Epoch 115/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0267 - acc: 0.9924 - val_loss: 0.4227 - val_acc: 0.9175\n",
      "Epoch 116/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0303 - acc: 0.9903 - val_loss: 0.3982 - val_acc: 0.9166\n",
      "Epoch 117/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0238 - acc: 0.9930 - val_loss: 0.4085 - val_acc: 0.9148\n",
      "Epoch 118/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0374 - acc: 0.9879 - val_loss: 0.4403 - val_acc: 0.9069\n",
      "Epoch 119/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0417 - acc: 0.9874 - val_loss: 0.4037 - val_acc: 0.9117\n",
      "Epoch 120/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0361 - acc: 0.9886 - val_loss: 0.4147 - val_acc: 0.9130\n",
      "Epoch 121/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0297 - acc: 0.9910 - val_loss: 0.3902 - val_acc: 0.9160\n",
      "Epoch 122/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0239 - acc: 0.9934 - val_loss: 0.4090 - val_acc: 0.9151\n",
      "Epoch 123/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0227 - acc: 0.9925 - val_loss: 0.4161 - val_acc: 0.9157\n",
      "Epoch 124/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0263 - acc: 0.9916 - val_loss: 0.4607 - val_acc: 0.9087\n",
      "Epoch 125/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0252 - acc: 0.9924 - val_loss: 0.4590 - val_acc: 0.9175\n",
      "Epoch 126/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0328 - acc: 0.9907 - val_loss: 0.4859 - val_acc: 0.9093\n",
      "Epoch 127/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0404 - acc: 0.9873 - val_loss: 0.4530 - val_acc: 0.9151\n",
      "Epoch 128/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0330 - acc: 0.9892 - val_loss: 0.4565 - val_acc: 0.9123\n",
      "Epoch 129/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0251 - acc: 0.9919 - val_loss: 0.4071 - val_acc: 0.9230\n",
      "Epoch 130/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0311 - acc: 0.9906 - val_loss: 0.4244 - val_acc: 0.9169\n",
      "Epoch 131/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0292 - acc: 0.9910 - val_loss: 0.4073 - val_acc: 0.9214\n",
      "Epoch 132/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0341 - acc: 0.9894 - val_loss: 0.4041 - val_acc: 0.9175\n",
      "Epoch 133/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0249 - acc: 0.9924 - val_loss: 0.4200 - val_acc: 0.9178\n",
      "Epoch 134/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0275 - acc: 0.9922 - val_loss: 0.4241 - val_acc: 0.9157\n",
      "Epoch 135/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0257 - acc: 0.9915 - val_loss: 0.4418 - val_acc: 0.9142\n",
      "Epoch 136/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0222 - acc: 0.9919 - val_loss: 0.4255 - val_acc: 0.9172\n",
      "Epoch 137/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0292 - acc: 0.9910 - val_loss: 0.4189 - val_acc: 0.9184\n",
      "Epoch 138/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0317 - acc: 0.9907 - val_loss: 0.4266 - val_acc: 0.9151\n",
      "Epoch 139/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0254 - acc: 0.9912 - val_loss: 0.4240 - val_acc: 0.9166\n",
      "Epoch 140/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0261 - acc: 0.9913 - val_loss: 0.4627 - val_acc: 0.9139\n",
      "Epoch 141/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0180 - acc: 0.9942 - val_loss: 0.4596 - val_acc: 0.9160\n",
      "Epoch 142/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0174 - acc: 0.9942 - val_loss: 0.4780 - val_acc: 0.9133\n",
      "Epoch 143/600\n",
      "6693/6693 [==============================] - 1s 113us/sample - loss: 0.0226 - acc: 0.9937 - val_loss: 0.4637 - val_acc: 0.9187\n",
      "Epoch 144/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0349 - acc: 0.9877 - val_loss: 0.4780 - val_acc: 0.9142\n",
      "Epoch 145/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0401 - acc: 0.9874 - val_loss: 0.4079 - val_acc: 0.9151\n",
      "Epoch 146/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0235 - acc: 0.9924 - val_loss: 0.4673 - val_acc: 0.9166\n",
      "Epoch 147/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0261 - acc: 0.9912 - val_loss: 0.4863 - val_acc: 0.9130\n",
      "Epoch 148/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0333 - acc: 0.9895 - val_loss: 0.4515 - val_acc: 0.9102\n",
      "Epoch 149/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0323 - acc: 0.9900 - val_loss: 0.4109 - val_acc: 0.9166\n",
      "Epoch 150/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0330 - acc: 0.9900 - val_loss: 0.4145 - val_acc: 0.9148\n",
      "Epoch 151/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0297 - acc: 0.9918 - val_loss: 0.4336 - val_acc: 0.9157\n",
      "Epoch 152/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0280 - acc: 0.9900 - val_loss: 0.4463 - val_acc: 0.9123\n",
      "Epoch 153/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0278 - acc: 0.9915 - val_loss: 0.4492 - val_acc: 0.9111\n",
      "Epoch 154/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0254 - acc: 0.9907 - val_loss: 0.4578 - val_acc: 0.9154\n",
      "Epoch 155/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0309 - acc: 0.9922 - val_loss: 0.4344 - val_acc: 0.9139\n",
      "Epoch 156/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0331 - acc: 0.9900 - val_loss: 0.4221 - val_acc: 0.9190\n",
      "Epoch 157/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0226 - acc: 0.9928 - val_loss: 0.4016 - val_acc: 0.9199\n",
      "Epoch 158/600\n",
      "6693/6693 [==============================] - 1s 105us/sample - loss: 0.0242 - acc: 0.9928 - val_loss: 0.4314 - val_acc: 0.9163\n",
      "Epoch 159/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0316 - acc: 0.9916 - val_loss: 0.3927 - val_acc: 0.9239\n",
      "Epoch 160/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0289 - acc: 0.9891 - val_loss: 0.3869 - val_acc: 0.9224\n",
      "Epoch 161/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0247 - acc: 0.9922 - val_loss: 0.3692 - val_acc: 0.9263\n",
      "Epoch 162/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0260 - acc: 0.9910 - val_loss: 0.4081 - val_acc: 0.9169\n",
      "Epoch 163/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0237 - acc: 0.9928 - val_loss: 0.3940 - val_acc: 0.9257\n",
      "Epoch 164/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0256 - acc: 0.9922 - val_loss: 0.4208 - val_acc: 0.9227\n",
      "Epoch 165/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0277 - acc: 0.9915 - val_loss: 0.4141 - val_acc: 0.9205\n",
      "Epoch 166/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0268 - acc: 0.9925 - val_loss: 0.4237 - val_acc: 0.9190\n",
      "Epoch 167/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0231 - acc: 0.9919 - val_loss: 0.4575 - val_acc: 0.9151\n",
      "Epoch 168/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0264 - acc: 0.9918 - val_loss: 0.4559 - val_acc: 0.9196\n",
      "Epoch 169/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0238 - acc: 0.9924 - val_loss: 0.4115 - val_acc: 0.9257\n",
      "Epoch 170/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0243 - acc: 0.9925 - val_loss: 0.4496 - val_acc: 0.9160\n",
      "Epoch 171/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0300 - acc: 0.9913 - val_loss: 0.4272 - val_acc: 0.9151\n",
      "Epoch 172/600\n",
      "6693/6693 [==============================] - 1s 110us/sample - loss: 0.0269 - acc: 0.9924 - val_loss: 0.4097 - val_acc: 0.9211\n",
      "Epoch 173/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0321 - acc: 0.9907 - val_loss: 0.4299 - val_acc: 0.9196\n",
      "Epoch 174/600\n",
      "6693/6693 [==============================] - 1s 109us/sample - loss: 0.0333 - acc: 0.9901 - val_loss: 0.4094 - val_acc: 0.9196\n",
      "Epoch 175/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0289 - acc: 0.9916 - val_loss: 0.4104 - val_acc: 0.9172\n",
      "Epoch 176/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0195 - acc: 0.9930 - val_loss: 0.3923 - val_acc: 0.9193\n",
      "Epoch 177/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0173 - acc: 0.9936 - val_loss: 0.4204 - val_acc: 0.9175\n",
      "Epoch 178/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0188 - acc: 0.9945 - val_loss: 0.4441 - val_acc: 0.9154\n",
      "Epoch 179/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0242 - acc: 0.9931 - val_loss: 0.4087 - val_acc: 0.9202\n",
      "Epoch 180/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0213 - acc: 0.9931 - val_loss: 0.4067 - val_acc: 0.9117\n",
      "Epoch 181/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0245 - acc: 0.9919 - val_loss: 0.4036 - val_acc: 0.9181\n",
      "Epoch 182/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0272 - acc: 0.9925 - val_loss: 0.3966 - val_acc: 0.9193\n",
      "Epoch 183/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0399 - acc: 0.9889 - val_loss: 0.3915 - val_acc: 0.9205\n",
      "Epoch 184/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0233 - acc: 0.9922 - val_loss: 0.3671 - val_acc: 0.9245\n",
      "Epoch 185/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0193 - acc: 0.9940 - val_loss: 0.3736 - val_acc: 0.9221\n",
      "Epoch 186/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0127 - acc: 0.9958 - val_loss: 0.4150 - val_acc: 0.9211\n",
      "Epoch 187/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0249 - acc: 0.9927 - val_loss: 0.3852 - val_acc: 0.9236\n",
      "Epoch 188/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0190 - acc: 0.9936 - val_loss: 0.3976 - val_acc: 0.9266\n",
      "Epoch 189/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0238 - acc: 0.9934 - val_loss: 0.4733 - val_acc: 0.9139\n",
      "Epoch 190/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0244 - acc: 0.9915 - val_loss: 0.4351 - val_acc: 0.9199\n",
      "Epoch 191/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0278 - acc: 0.9909 - val_loss: 0.4562 - val_acc: 0.9105\n",
      "Epoch 192/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0405 - acc: 0.9870 - val_loss: 0.3818 - val_acc: 0.9205\n",
      "Epoch 193/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0244 - acc: 0.9927 - val_loss: 0.3961 - val_acc: 0.9196\n",
      "Epoch 194/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0214 - acc: 0.9930 - val_loss: 0.3898 - val_acc: 0.9205\n",
      "Epoch 195/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0200 - acc: 0.9943 - val_loss: 0.3947 - val_acc: 0.9233\n",
      "Epoch 196/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0162 - acc: 0.9940 - val_loss: 0.3996 - val_acc: 0.9248\n",
      "Epoch 197/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0179 - acc: 0.9936 - val_loss: 0.4900 - val_acc: 0.9145\n",
      "Epoch 198/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0181 - acc: 0.9937 - val_loss: 0.4825 - val_acc: 0.9157\n",
      "Epoch 199/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0250 - acc: 0.9922 - val_loss: 0.4840 - val_acc: 0.9160\n",
      "Epoch 200/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0344 - acc: 0.9904 - val_loss: 0.4292 - val_acc: 0.9172\n",
      "Epoch 201/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0244 - acc: 0.9930 - val_loss: 0.4060 - val_acc: 0.9217\n",
      "Epoch 202/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0198 - acc: 0.9939 - val_loss: 0.4510 - val_acc: 0.9157\n",
      "Epoch 203/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0167 - acc: 0.9945 - val_loss: 0.4566 - val_acc: 0.9160\n",
      "Epoch 204/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0204 - acc: 0.9934 - val_loss: 0.4632 - val_acc: 0.9169\n",
      "Epoch 205/600\n",
      "6693/6693 [==============================] - 1s 109us/sample - loss: 0.0186 - acc: 0.9937 - val_loss: 0.4391 - val_acc: 0.9145\n",
      "Epoch 206/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0244 - acc: 0.9921 - val_loss: 0.4168 - val_acc: 0.9233\n",
      "Epoch 207/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0247 - acc: 0.9934 - val_loss: 0.4495 - val_acc: 0.9175\n",
      "Epoch 208/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0201 - acc: 0.9936 - val_loss: 0.4565 - val_acc: 0.9166\n",
      "Epoch 209/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0158 - acc: 0.9946 - val_loss: 0.4797 - val_acc: 0.9181\n",
      "Epoch 210/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0117 - acc: 0.9960 - val_loss: 0.4598 - val_acc: 0.9199\n",
      "Epoch 211/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0121 - acc: 0.9963 - val_loss: 0.4778 - val_acc: 0.9205\n",
      "Epoch 212/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0142 - acc: 0.9957 - val_loss: 0.4814 - val_acc: 0.9221\n",
      "Epoch 213/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0213 - acc: 0.9948 - val_loss: 0.4753 - val_acc: 0.9172\n",
      "Epoch 214/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0126 - acc: 0.9961 - val_loss: 0.4378 - val_acc: 0.9242\n",
      "Epoch 215/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0218 - acc: 0.9945 - val_loss: 0.4587 - val_acc: 0.9205\n",
      "Epoch 216/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0126 - acc: 0.9958 - val_loss: 0.4665 - val_acc: 0.9245\n",
      "Epoch 217/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.4628 - val_acc: 0.9217\n",
      "Epoch 218/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0239 - acc: 0.9940 - val_loss: 0.4565 - val_acc: 0.9151\n",
      "Epoch 219/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0314 - acc: 0.9922 - val_loss: 0.4787 - val_acc: 0.9111\n",
      "Epoch 220/600\n",
      "6693/6693 [==============================] - 1s 95us/sample - loss: 0.0323 - acc: 0.9907 - val_loss: 0.5188 - val_acc: 0.9057\n",
      "Epoch 221/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0243 - acc: 0.9933 - val_loss: 0.4669 - val_acc: 0.9123\n",
      "Epoch 222/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0270 - acc: 0.9936 - val_loss: 0.4417 - val_acc: 0.9175\n",
      "Epoch 223/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0235 - acc: 0.9931 - val_loss: 0.4308 - val_acc: 0.9178\n",
      "Epoch 224/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0194 - acc: 0.9933 - val_loss: 0.4694 - val_acc: 0.9157\n",
      "Epoch 225/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0229 - acc: 0.9931 - val_loss: 0.4626 - val_acc: 0.9160\n",
      "Epoch 226/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0170 - acc: 0.9940 - val_loss: 0.4321 - val_acc: 0.9205\n",
      "Epoch 227/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0188 - acc: 0.9940 - val_loss: 0.4950 - val_acc: 0.9160\n",
      "Epoch 228/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0183 - acc: 0.9946 - val_loss: 0.4917 - val_acc: 0.9154\n",
      "Epoch 229/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0200 - acc: 0.9939 - val_loss: 0.4463 - val_acc: 0.9208\n",
      "Epoch 230/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0191 - acc: 0.9933 - val_loss: 0.4283 - val_acc: 0.9248\n",
      "Epoch 231/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0172 - acc: 0.9939 - val_loss: 0.4588 - val_acc: 0.9257\n",
      "Epoch 232/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0255 - acc: 0.9930 - val_loss: 0.4701 - val_acc: 0.9166\n",
      "Epoch 233/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0213 - acc: 0.9931 - val_loss: 0.4729 - val_acc: 0.9199\n",
      "Epoch 234/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0215 - acc: 0.9925 - val_loss: 0.4554 - val_acc: 0.9233\n",
      "Epoch 235/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0253 - acc: 0.9940 - val_loss: 0.4580 - val_acc: 0.9123\n",
      "Epoch 236/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0202 - acc: 0.9931 - val_loss: 0.4748 - val_acc: 0.9178\n",
      "Epoch 237/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0194 - acc: 0.9939 - val_loss: 0.4751 - val_acc: 0.9166\n",
      "Epoch 238/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0191 - acc: 0.9940 - val_loss: 0.5237 - val_acc: 0.9133\n",
      "Epoch 239/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0144 - acc: 0.9954 - val_loss: 0.5034 - val_acc: 0.9202\n",
      "Epoch 240/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0173 - acc: 0.9949 - val_loss: 0.5029 - val_acc: 0.9175\n",
      "Epoch 241/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0196 - acc: 0.9942 - val_loss: 0.5077 - val_acc: 0.9178\n",
      "Epoch 242/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0154 - acc: 0.9949 - val_loss: 0.5060 - val_acc: 0.9136\n",
      "Epoch 243/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0165 - acc: 0.9960 - val_loss: 0.5431 - val_acc: 0.9160\n",
      "Epoch 244/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0264 - acc: 0.9915 - val_loss: 0.5268 - val_acc: 0.9087\n",
      "Epoch 245/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0308 - acc: 0.9907 - val_loss: 0.4767 - val_acc: 0.9123\n",
      "Epoch 246/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0174 - acc: 0.9940 - val_loss: 0.4486 - val_acc: 0.9199\n",
      "Epoch 247/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0119 - acc: 0.9957 - val_loss: 0.4553 - val_acc: 0.9227\n",
      "Epoch 248/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0154 - acc: 0.9946 - val_loss: 0.4832 - val_acc: 0.9199\n",
      "Epoch 249/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0153 - acc: 0.9951 - val_loss: 0.4923 - val_acc: 0.9190\n",
      "Epoch 250/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0218 - acc: 0.9933 - val_loss: 0.4764 - val_acc: 0.9187\n",
      "Epoch 251/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0194 - acc: 0.9934 - val_loss: 0.4899 - val_acc: 0.9154\n",
      "Epoch 252/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0207 - acc: 0.9918 - val_loss: 0.4463 - val_acc: 0.9199\n",
      "Epoch 253/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0201 - acc: 0.9931 - val_loss: 0.4664 - val_acc: 0.9217\n",
      "Epoch 254/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0176 - acc: 0.9945 - val_loss: 0.4624 - val_acc: 0.9169\n",
      "Epoch 255/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0150 - acc: 0.9952 - val_loss: 0.4306 - val_acc: 0.9190\n",
      "Epoch 256/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0209 - acc: 0.9945 - val_loss: 0.4083 - val_acc: 0.9230\n",
      "Epoch 257/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0168 - acc: 0.9946 - val_loss: 0.4248 - val_acc: 0.9248\n",
      "Epoch 258/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0159 - acc: 0.9954 - val_loss: 0.4433 - val_acc: 0.9233\n",
      "Epoch 259/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0177 - acc: 0.9940 - val_loss: 0.4272 - val_acc: 0.9257\n",
      "Epoch 260/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0198 - acc: 0.9934 - val_loss: 0.4788 - val_acc: 0.9178\n",
      "Epoch 261/600\n",
      "6693/6693 [==============================] - 1s 105us/sample - loss: 0.0192 - acc: 0.9937 - val_loss: 0.4469 - val_acc: 0.9190\n",
      "Epoch 262/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0150 - acc: 0.9946 - val_loss: 0.4657 - val_acc: 0.9172\n",
      "Epoch 263/600\n",
      "6693/6693 [==============================] - 1s 115us/sample - loss: 0.0186 - acc: 0.9939 - val_loss: 0.4531 - val_acc: 0.9199\n",
      "Epoch 264/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0156 - acc: 0.9955 - val_loss: 0.4397 - val_acc: 0.9151\n",
      "Epoch 265/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0166 - acc: 0.9949 - val_loss: 0.4780 - val_acc: 0.9163\n",
      "Epoch 266/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0150 - acc: 0.9945 - val_loss: 0.4964 - val_acc: 0.9196\n",
      "Epoch 267/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0177 - acc: 0.9936 - val_loss: 0.4815 - val_acc: 0.9151\n",
      "Epoch 268/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0190 - acc: 0.9930 - val_loss: 0.4820 - val_acc: 0.9187\n",
      "Epoch 269/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0197 - acc: 0.9939 - val_loss: 0.4978 - val_acc: 0.9166\n",
      "Epoch 270/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0230 - acc: 0.9924 - val_loss: 0.4953 - val_acc: 0.9151\n",
      "Epoch 271/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0205 - acc: 0.9934 - val_loss: 0.4627 - val_acc: 0.9166\n",
      "Epoch 272/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0192 - acc: 0.9949 - val_loss: 0.4311 - val_acc: 0.9254\n",
      "Epoch 273/600\n",
      "6693/6693 [==============================] - 1s 109us/sample - loss: 0.0205 - acc: 0.9940 - val_loss: 0.4468 - val_acc: 0.9242\n",
      "Epoch 274/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0211 - acc: 0.9931 - val_loss: 0.4341 - val_acc: 0.9190\n",
      "Epoch 275/600\n",
      "6693/6693 [==============================] - 1s 120us/sample - loss: 0.0149 - acc: 0.9948 - val_loss: 0.4562 - val_acc: 0.9208\n",
      "Epoch 276/600\n",
      "6693/6693 [==============================] - 1s 117us/sample - loss: 0.0198 - acc: 0.9939 - val_loss: 0.4595 - val_acc: 0.9163\n",
      "Epoch 277/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0170 - acc: 0.9952 - val_loss: 0.4897 - val_acc: 0.9163\n",
      "Epoch 278/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0140 - acc: 0.9955 - val_loss: 0.4628 - val_acc: 0.9199\n",
      "Epoch 279/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0148 - acc: 0.9946 - val_loss: 0.4731 - val_acc: 0.9196\n",
      "Epoch 280/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0143 - acc: 0.9958 - val_loss: 0.4500 - val_acc: 0.9269\n",
      "Epoch 281/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0113 - acc: 0.9960 - val_loss: 0.4764 - val_acc: 0.9196\n",
      "Epoch 282/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0128 - acc: 0.9952 - val_loss: 0.4900 - val_acc: 0.9178\n",
      "Epoch 283/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0125 - acc: 0.9951 - val_loss: 0.5217 - val_acc: 0.9208\n",
      "Epoch 284/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0104 - acc: 0.9969 - val_loss: 0.5053 - val_acc: 0.9202\n",
      "Epoch 285/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0159 - acc: 0.9952 - val_loss: 0.5314 - val_acc: 0.9181\n",
      "Epoch 286/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0254 - acc: 0.9924 - val_loss: 0.4673 - val_acc: 0.9205\n",
      "Epoch 287/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0229 - acc: 0.9937 - val_loss: 0.4695 - val_acc: 0.9166\n",
      "Epoch 288/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0163 - acc: 0.9945 - val_loss: 0.4400 - val_acc: 0.9236\n",
      "Epoch 289/600\n",
      "6693/6693 [==============================] - 1s 110us/sample - loss: 0.0162 - acc: 0.9948 - val_loss: 0.4326 - val_acc: 0.9260\n",
      "Epoch 290/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0280 - acc: 0.9918 - val_loss: 0.4883 - val_acc: 0.9178\n",
      "Epoch 291/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0202 - acc: 0.9939 - val_loss: 0.4078 - val_acc: 0.9242\n",
      "Epoch 292/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0177 - acc: 0.9940 - val_loss: 0.4836 - val_acc: 0.9187\n",
      "Epoch 293/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0234 - acc: 0.9925 - val_loss: 0.4440 - val_acc: 0.9211\n",
      "Epoch 294/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0222 - acc: 0.9936 - val_loss: 0.4124 - val_acc: 0.9224\n",
      "Epoch 295/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0208 - acc: 0.9936 - val_loss: 0.4799 - val_acc: 0.9166\n",
      "Epoch 296/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0252 - acc: 0.9922 - val_loss: 0.4658 - val_acc: 0.9193\n",
      "Epoch 297/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0177 - acc: 0.9939 - val_loss: 0.4700 - val_acc: 0.9187\n",
      "Epoch 298/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0136 - acc: 0.9948 - val_loss: 0.4641 - val_acc: 0.9227\n",
      "Epoch 299/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0189 - acc: 0.9939 - val_loss: 0.4733 - val_acc: 0.9202\n",
      "Epoch 300/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0110 - acc: 0.9957 - val_loss: 0.4634 - val_acc: 0.9169\n",
      "Epoch 301/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0137 - acc: 0.9948 - val_loss: 0.5120 - val_acc: 0.9175\n",
      "Epoch 302/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0174 - acc: 0.9942 - val_loss: 0.4708 - val_acc: 0.9175\n",
      "Epoch 303/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0177 - acc: 0.9948 - val_loss: 0.5056 - val_acc: 0.9123\n",
      "Epoch 304/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0213 - acc: 0.9936 - val_loss: 0.4563 - val_acc: 0.9224\n",
      "Epoch 305/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0255 - acc: 0.9922 - val_loss: 0.4573 - val_acc: 0.9154\n",
      "Epoch 306/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0136 - acc: 0.9943 - val_loss: 0.4715 - val_acc: 0.9208\n",
      "Epoch 307/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0096 - acc: 0.9966 - val_loss: 0.4725 - val_acc: 0.9224\n",
      "Epoch 308/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0239 - acc: 0.9928 - val_loss: 0.5371 - val_acc: 0.9142\n",
      "Epoch 309/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0222 - acc: 0.9936 - val_loss: 0.4785 - val_acc: 0.9142\n",
      "Epoch 310/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0203 - acc: 0.9934 - val_loss: 0.5078 - val_acc: 0.9148\n",
      "Epoch 311/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0210 - acc: 0.9934 - val_loss: 0.4597 - val_acc: 0.9205\n",
      "Epoch 312/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0136 - acc: 0.9960 - val_loss: 0.4640 - val_acc: 0.9172\n",
      "Epoch 313/600\n",
      "6693/6693 [==============================] - 1s 112us/sample - loss: 0.0149 - acc: 0.9960 - val_loss: 0.4568 - val_acc: 0.9190\n",
      "Epoch 314/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0164 - acc: 0.9946 - val_loss: 0.4826 - val_acc: 0.9181\n",
      "Epoch 315/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0160 - acc: 0.9948 - val_loss: 0.4712 - val_acc: 0.9196\n",
      "Epoch 316/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0135 - acc: 0.9960 - val_loss: 0.4543 - val_acc: 0.9224\n",
      "Epoch 317/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0135 - acc: 0.9955 - val_loss: 0.4883 - val_acc: 0.9202\n",
      "Epoch 318/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0214 - acc: 0.9943 - val_loss: 0.4606 - val_acc: 0.9230\n",
      "Epoch 319/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0225 - acc: 0.9934 - val_loss: 0.4626 - val_acc: 0.9202\n",
      "Epoch 320/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0208 - acc: 0.9939 - val_loss: 0.4442 - val_acc: 0.9224\n",
      "Epoch 321/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0154 - acc: 0.9948 - val_loss: 0.4593 - val_acc: 0.9224\n",
      "Epoch 322/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0086 - acc: 0.9966 - val_loss: 0.4614 - val_acc: 0.9248\n",
      "Epoch 323/600\n",
      "6693/6693 [==============================] - 1s 121us/sample - loss: 0.0121 - acc: 0.9969 - val_loss: 0.5098 - val_acc: 0.9187\n",
      "Epoch 324/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0236 - acc: 0.9931 - val_loss: 0.4811 - val_acc: 0.9187\n",
      "Epoch 325/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0128 - acc: 0.9961 - val_loss: 0.4587 - val_acc: 0.9217\n",
      "Epoch 326/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0143 - acc: 0.9955 - val_loss: 0.4763 - val_acc: 0.9181\n",
      "Epoch 327/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0181 - acc: 0.9948 - val_loss: 0.4672 - val_acc: 0.9233\n",
      "Epoch 328/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0180 - acc: 0.9940 - val_loss: 0.4992 - val_acc: 0.9227\n",
      "Epoch 329/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0137 - acc: 0.9948 - val_loss: 0.4983 - val_acc: 0.9172\n",
      "Epoch 330/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0093 - acc: 0.9960 - val_loss: 0.5018 - val_acc: 0.9178\n",
      "Epoch 331/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0187 - acc: 0.9952 - val_loss: 0.5439 - val_acc: 0.9166\n",
      "Epoch 332/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0180 - acc: 0.9949 - val_loss: 0.4965 - val_acc: 0.9217\n",
      "Epoch 333/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0210 - acc: 0.9930 - val_loss: 0.4741 - val_acc: 0.9224\n",
      "Epoch 334/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0167 - acc: 0.9946 - val_loss: 0.4804 - val_acc: 0.9233\n",
      "Epoch 335/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0072 - acc: 0.9969 - val_loss: 0.4806 - val_acc: 0.9242\n",
      "Epoch 336/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0152 - acc: 0.9951 - val_loss: 0.4755 - val_acc: 0.9214\n",
      "Epoch 337/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0123 - acc: 0.9955 - val_loss: 0.4904 - val_acc: 0.9251\n",
      "Epoch 338/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0092 - acc: 0.9969 - val_loss: 0.5076 - val_acc: 0.9211\n",
      "Epoch 339/600\n",
      "6693/6693 [==============================] - 1s 105us/sample - loss: 0.0162 - acc: 0.9949 - val_loss: 0.5080 - val_acc: 0.9245\n",
      "Epoch 340/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0206 - acc: 0.9946 - val_loss: 0.5223 - val_acc: 0.9205\n",
      "Epoch 341/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0121 - acc: 0.9957 - val_loss: 0.4768 - val_acc: 0.9245\n",
      "Epoch 342/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0118 - acc: 0.9969 - val_loss: 0.4482 - val_acc: 0.9263\n",
      "Epoch 343/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0152 - acc: 0.9954 - val_loss: 0.4870 - val_acc: 0.9208\n",
      "Epoch 344/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0198 - acc: 0.9936 - val_loss: 0.4597 - val_acc: 0.9199\n",
      "Epoch 345/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0189 - acc: 0.9946 - val_loss: 0.4439 - val_acc: 0.9202\n",
      "Epoch 346/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0175 - acc: 0.9946 - val_loss: 0.4492 - val_acc: 0.9196\n",
      "Epoch 347/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0101 - acc: 0.9969 - val_loss: 0.4419 - val_acc: 0.9221\n",
      "Epoch 348/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0158 - acc: 0.9948 - val_loss: 0.4797 - val_acc: 0.9178\n",
      "Epoch 349/600\n",
      "6693/6693 [==============================] - 1s 105us/sample - loss: 0.0109 - acc: 0.9957 - val_loss: 0.4614 - val_acc: 0.9217\n",
      "Epoch 350/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0142 - acc: 0.9958 - val_loss: 0.4746 - val_acc: 0.9211\n",
      "Epoch 351/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0143 - acc: 0.9954 - val_loss: 0.4083 - val_acc: 0.9269\n",
      "Epoch 352/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0147 - acc: 0.9955 - val_loss: 0.4279 - val_acc: 0.9230\n",
      "Epoch 353/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0165 - acc: 0.9943 - val_loss: 0.4776 - val_acc: 0.9217\n",
      "Epoch 354/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0195 - acc: 0.9933 - val_loss: 0.4611 - val_acc: 0.9178\n",
      "Epoch 355/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0177 - acc: 0.9946 - val_loss: 0.4717 - val_acc: 0.9208\n",
      "Epoch 356/600\n",
      "6693/6693 [==============================] - 1s 109us/sample - loss: 0.0113 - acc: 0.9954 - val_loss: 0.4660 - val_acc: 0.9187\n",
      "Epoch 357/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0131 - acc: 0.9948 - val_loss: 0.4593 - val_acc: 0.9233\n",
      "Epoch 358/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0102 - acc: 0.9964 - val_loss: 0.4902 - val_acc: 0.9193\n",
      "Epoch 359/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0241 - acc: 0.9927 - val_loss: 0.4823 - val_acc: 0.9239\n",
      "Epoch 360/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0173 - acc: 0.9937 - val_loss: 0.4563 - val_acc: 0.9217\n",
      "Epoch 361/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0146 - acc: 0.9955 - val_loss: 0.4739 - val_acc: 0.9239\n",
      "Epoch 362/600\n",
      "6693/6693 [==============================] - 1s 108us/sample - loss: 0.0125 - acc: 0.9955 - val_loss: 0.4924 - val_acc: 0.9227\n",
      "Epoch 363/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0171 - acc: 0.9946 - val_loss: 0.4904 - val_acc: 0.9208\n",
      "Epoch 364/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0192 - acc: 0.9934 - val_loss: 0.4890 - val_acc: 0.9196\n",
      "Epoch 365/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0240 - acc: 0.9945 - val_loss: 0.5179 - val_acc: 0.9172\n",
      "Epoch 366/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0135 - acc: 0.9949 - val_loss: 0.5060 - val_acc: 0.9272\n",
      "Epoch 367/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0165 - acc: 0.9951 - val_loss: 0.4841 - val_acc: 0.9163\n",
      "Epoch 368/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0108 - acc: 0.9952 - val_loss: 0.4590 - val_acc: 0.9245\n",
      "Epoch 369/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0082 - acc: 0.9973 - val_loss: 0.5250 - val_acc: 0.9248\n",
      "Epoch 370/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0143 - acc: 0.9957 - val_loss: 0.5433 - val_acc: 0.9211\n",
      "Epoch 371/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0219 - acc: 0.9943 - val_loss: 0.5012 - val_acc: 0.9239\n",
      "Epoch 372/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0249 - acc: 0.9924 - val_loss: 0.4692 - val_acc: 0.9193\n",
      "Epoch 373/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0167 - acc: 0.9952 - val_loss: 0.4614 - val_acc: 0.9272\n",
      "Epoch 374/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0138 - acc: 0.9951 - val_loss: 0.4554 - val_acc: 0.9208\n",
      "Epoch 375/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0168 - acc: 0.9952 - val_loss: 0.4916 - val_acc: 0.9239\n",
      "Epoch 376/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0174 - acc: 0.9948 - val_loss: 0.4819 - val_acc: 0.9211\n",
      "Epoch 377/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0117 - acc: 0.9969 - val_loss: 0.4969 - val_acc: 0.9239\n",
      "Epoch 378/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0103 - acc: 0.9967 - val_loss: 0.4891 - val_acc: 0.9260\n",
      "Epoch 379/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0105 - acc: 0.9963 - val_loss: 0.4758 - val_acc: 0.9278\n",
      "Epoch 380/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0179 - acc: 0.9955 - val_loss: 0.4715 - val_acc: 0.9242\n",
      "Epoch 381/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0116 - acc: 0.9961 - val_loss: 0.4690 - val_acc: 0.9281\n",
      "Epoch 382/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0123 - acc: 0.9958 - val_loss: 0.5013 - val_acc: 0.9269\n",
      "Epoch 383/600\n",
      "6693/6693 [==============================] - 1s 114us/sample - loss: 0.0130 - acc: 0.9951 - val_loss: 0.5033 - val_acc: 0.9239\n",
      "Epoch 384/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0125 - acc: 0.9967 - val_loss: 0.5346 - val_acc: 0.9224\n",
      "Epoch 385/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0182 - acc: 0.9952 - val_loss: 0.4944 - val_acc: 0.9290\n",
      "Epoch 386/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0168 - acc: 0.9945 - val_loss: 0.5279 - val_acc: 0.9196\n",
      "Epoch 387/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0216 - acc: 0.9927 - val_loss: 0.4641 - val_acc: 0.9217\n",
      "Epoch 388/600\n",
      "6693/6693 [==============================] - 1s 119us/sample - loss: 0.0141 - acc: 0.9958 - val_loss: 0.4828 - val_acc: 0.9227\n",
      "Epoch 389/600\n",
      "6693/6693 [==============================] - 1s 116us/sample - loss: 0.0164 - acc: 0.9955 - val_loss: 0.4891 - val_acc: 0.9217\n",
      "Epoch 390/600\n",
      "6693/6693 [==============================] - 1s 115us/sample - loss: 0.0133 - acc: 0.9960 - val_loss: 0.4998 - val_acc: 0.9224\n",
      "Epoch 391/600\n",
      "6693/6693 [==============================] - 1s 119us/sample - loss: 0.0089 - acc: 0.9961 - val_loss: 0.5349 - val_acc: 0.9248\n",
      "Epoch 392/600\n",
      "6693/6693 [==============================] - 1s 116us/sample - loss: 0.0112 - acc: 0.9952 - val_loss: 0.4798 - val_acc: 0.9245\n",
      "Epoch 393/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0129 - acc: 0.9957 - val_loss: 0.5096 - val_acc: 0.9230\n",
      "Epoch 394/600\n",
      "6693/6693 [==============================] - 1s 108us/sample - loss: 0.0104 - acc: 0.9967 - val_loss: 0.4967 - val_acc: 0.9236\n",
      "Epoch 395/600\n",
      "6693/6693 [==============================] - 1s 105us/sample - loss: 0.0166 - acc: 0.9954 - val_loss: 0.5224 - val_acc: 0.9208\n",
      "Epoch 396/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0148 - acc: 0.9948 - val_loss: 0.5321 - val_acc: 0.9233\n",
      "Epoch 397/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0121 - acc: 0.9964 - val_loss: 0.4767 - val_acc: 0.9211\n",
      "Epoch 398/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0112 - acc: 0.9960 - val_loss: 0.5062 - val_acc: 0.9281\n",
      "Epoch 399/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0152 - acc: 0.9943 - val_loss: 0.4967 - val_acc: 0.9251\n",
      "Epoch 400/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0195 - acc: 0.9942 - val_loss: 0.4861 - val_acc: 0.9224\n",
      "Epoch 401/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0236 - acc: 0.9939 - val_loss: 0.5217 - val_acc: 0.9190\n",
      "Epoch 402/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.4765 - val_acc: 0.9224\n",
      "Epoch 403/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0149 - acc: 0.9946 - val_loss: 0.4820 - val_acc: 0.9251\n",
      "Epoch 404/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0136 - acc: 0.9954 - val_loss: 0.4922 - val_acc: 0.9257\n",
      "Epoch 405/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0095 - acc: 0.9961 - val_loss: 0.4866 - val_acc: 0.9227\n",
      "Epoch 406/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0161 - acc: 0.9946 - val_loss: 0.5043 - val_acc: 0.9245\n",
      "Epoch 407/600\n",
      "6693/6693 [==============================] - 1s 115us/sample - loss: 0.0153 - acc: 0.9955 - val_loss: 0.4885 - val_acc: 0.9233\n",
      "Epoch 408/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0126 - acc: 0.9943 - val_loss: 0.4592 - val_acc: 0.9263\n",
      "Epoch 409/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0229 - acc: 0.9930 - val_loss: 0.4730 - val_acc: 0.9233\n",
      "Epoch 410/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0146 - acc: 0.9948 - val_loss: 0.4411 - val_acc: 0.9272\n",
      "Epoch 411/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0131 - acc: 0.9964 - val_loss: 0.4659 - val_acc: 0.9248\n",
      "Epoch 412/600\n",
      "6693/6693 [==============================] - 1s 114us/sample - loss: 0.0112 - acc: 0.9957 - val_loss: 0.5252 - val_acc: 0.9217\n",
      "Epoch 413/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0074 - acc: 0.9973 - val_loss: 0.5028 - val_acc: 0.9230\n",
      "Epoch 414/600\n",
      "6693/6693 [==============================] - 1s 115us/sample - loss: 0.0105 - acc: 0.9964 - val_loss: 0.5304 - val_acc: 0.9242\n",
      "Epoch 415/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0228 - acc: 0.9940 - val_loss: 0.5272 - val_acc: 0.9187\n",
      "Epoch 416/600\n",
      "6693/6693 [==============================] - 1s 109us/sample - loss: 0.0235 - acc: 0.9919 - val_loss: 0.4591 - val_acc: 0.9211\n",
      "Epoch 417/600\n",
      "6693/6693 [==============================] - 1s 108us/sample - loss: 0.0184 - acc: 0.9949 - val_loss: 0.4910 - val_acc: 0.9202\n",
      "Epoch 418/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0175 - acc: 0.9930 - val_loss: 0.5027 - val_acc: 0.9175\n",
      "Epoch 419/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0187 - acc: 0.9936 - val_loss: 0.5215 - val_acc: 0.9178\n",
      "Epoch 420/600\n",
      "6693/6693 [==============================] - 1s 96us/sample - loss: 0.0177 - acc: 0.9940 - val_loss: 0.4591 - val_acc: 0.9211\n",
      "Epoch 421/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0147 - acc: 0.9949 - val_loss: 0.4553 - val_acc: 0.9236\n",
      "Epoch 422/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0110 - acc: 0.9961 - val_loss: 0.4602 - val_acc: 0.9208\n",
      "Epoch 423/600\n",
      "6693/6693 [==============================] - 1s 119us/sample - loss: 0.0091 - acc: 0.9964 - val_loss: 0.4871 - val_acc: 0.9181\n",
      "Epoch 424/600\n",
      "6693/6693 [==============================] - 1s 108us/sample - loss: 0.0206 - acc: 0.9952 - val_loss: 0.4544 - val_acc: 0.9193\n",
      "Epoch 425/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0121 - acc: 0.9957 - val_loss: 0.4650 - val_acc: 0.9205\n",
      "Epoch 426/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0150 - acc: 0.9969 - val_loss: 0.4771 - val_acc: 0.9193\n",
      "Epoch 427/600\n",
      "6693/6693 [==============================] - 1s 105us/sample - loss: 0.0158 - acc: 0.9951 - val_loss: 0.4720 - val_acc: 0.9175\n",
      "Epoch 428/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0165 - acc: 0.9955 - val_loss: 0.4703 - val_acc: 0.9202\n",
      "Epoch 429/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0133 - acc: 0.9951 - val_loss: 0.4735 - val_acc: 0.9224\n",
      "Epoch 430/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0132 - acc: 0.9952 - val_loss: 0.4730 - val_acc: 0.9239\n",
      "Epoch 431/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0108 - acc: 0.9964 - val_loss: 0.5145 - val_acc: 0.9224\n",
      "Epoch 432/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0104 - acc: 0.9954 - val_loss: 0.5097 - val_acc: 0.9190\n",
      "Epoch 433/600\n",
      "6693/6693 [==============================] - 1s 110us/sample - loss: 0.0107 - acc: 0.9958 - val_loss: 0.5116 - val_acc: 0.9221\n",
      "Epoch 434/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0180 - acc: 0.9948 - val_loss: 0.4848 - val_acc: 0.9260\n",
      "Epoch 435/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0119 - acc: 0.9951 - val_loss: 0.5358 - val_acc: 0.9233\n",
      "Epoch 436/600\n",
      "6693/6693 [==============================] - 1s 115us/sample - loss: 0.0136 - acc: 0.9955 - val_loss: 0.5035 - val_acc: 0.9251\n",
      "Epoch 437/600\n",
      "6693/6693 [==============================] - 1s 106us/sample - loss: 0.0139 - acc: 0.9955 - val_loss: 0.5053 - val_acc: 0.9217\n",
      "Epoch 438/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0124 - acc: 0.9961 - val_loss: 0.4959 - val_acc: 0.9248\n",
      "Epoch 439/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0216 - acc: 0.9942 - val_loss: 0.5048 - val_acc: 0.9199\n",
      "Epoch 440/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0150 - acc: 0.9945 - val_loss: 0.4649 - val_acc: 0.9245\n",
      "Epoch 441/600\n",
      "6693/6693 [==============================] - 1s 117us/sample - loss: 0.0154 - acc: 0.9951 - val_loss: 0.5018 - val_acc: 0.9160\n",
      "Epoch 442/600\n",
      "6693/6693 [==============================] - 1s 108us/sample - loss: 0.0165 - acc: 0.9942 - val_loss: 0.4700 - val_acc: 0.9245\n",
      "Epoch 443/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0116 - acc: 0.9964 - val_loss: 0.4746 - val_acc: 0.9236\n",
      "Epoch 444/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0077 - acc: 0.9975 - val_loss: 0.4830 - val_acc: 0.9278\n",
      "Epoch 445/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0123 - acc: 0.9960 - val_loss: 0.4983 - val_acc: 0.9263\n",
      "Epoch 446/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0205 - acc: 0.9949 - val_loss: 0.4649 - val_acc: 0.9290\n",
      "Epoch 447/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0101 - acc: 0.9967 - val_loss: 0.5128 - val_acc: 0.9254\n",
      "Epoch 448/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0100 - acc: 0.9963 - val_loss: 0.5215 - val_acc: 0.9257\n",
      "Epoch 449/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0150 - acc: 0.9954 - val_loss: 0.5072 - val_acc: 0.9221\n",
      "Epoch 450/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0106 - acc: 0.9969 - val_loss: 0.5188 - val_acc: 0.9257\n",
      "Epoch 451/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0141 - acc: 0.9970 - val_loss: 0.5119 - val_acc: 0.9266\n",
      "Epoch 452/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0118 - acc: 0.9963 - val_loss: 0.5798 - val_acc: 0.9181\n",
      "Epoch 453/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0123 - acc: 0.9952 - val_loss: 0.5630 - val_acc: 0.9190\n",
      "Epoch 454/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0131 - acc: 0.9967 - val_loss: 0.4996 - val_acc: 0.9245\n",
      "Epoch 455/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0144 - acc: 0.9951 - val_loss: 0.4742 - val_acc: 0.9227\n",
      "Epoch 456/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0079 - acc: 0.9972 - val_loss: 0.5316 - val_acc: 0.9254\n",
      "Epoch 457/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0066 - acc: 0.9979 - val_loss: 0.4928 - val_acc: 0.9302\n",
      "Epoch 458/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0113 - acc: 0.9967 - val_loss: 0.4987 - val_acc: 0.9263\n",
      "Epoch 459/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0136 - acc: 0.9949 - val_loss: 0.5144 - val_acc: 0.9202\n",
      "Epoch 460/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0146 - acc: 0.9957 - val_loss: 0.5248 - val_acc: 0.9190\n",
      "Epoch 461/600\n",
      "6693/6693 [==============================] - 1s 119us/sample - loss: 0.0130 - acc: 0.9957 - val_loss: 0.5545 - val_acc: 0.9233\n",
      "Epoch 462/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0159 - acc: 0.9942 - val_loss: 0.5017 - val_acc: 0.9263\n",
      "Epoch 463/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0131 - acc: 0.9948 - val_loss: 0.4911 - val_acc: 0.9299\n",
      "Epoch 464/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0136 - acc: 0.9958 - val_loss: 0.5073 - val_acc: 0.9230\n",
      "Epoch 465/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0158 - acc: 0.9948 - val_loss: 0.5497 - val_acc: 0.9208\n",
      "Epoch 466/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0142 - acc: 0.9955 - val_loss: 0.5536 - val_acc: 0.9236\n",
      "Epoch 467/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0187 - acc: 0.9946 - val_loss: 0.4924 - val_acc: 0.9239\n",
      "Epoch 468/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0166 - acc: 0.9940 - val_loss: 0.5161 - val_acc: 0.9217\n",
      "Epoch 469/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0099 - acc: 0.9955 - val_loss: 0.5010 - val_acc: 0.9278\n",
      "Epoch 470/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0123 - acc: 0.9955 - val_loss: 0.5180 - val_acc: 0.9217\n",
      "Epoch 471/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0101 - acc: 0.9957 - val_loss: 0.5842 - val_acc: 0.9178\n",
      "Epoch 472/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0130 - acc: 0.9963 - val_loss: 0.5407 - val_acc: 0.9230\n",
      "Epoch 473/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0121 - acc: 0.9958 - val_loss: 0.5468 - val_acc: 0.9227\n",
      "Epoch 474/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0181 - acc: 0.9939 - val_loss: 0.5471 - val_acc: 0.9248\n",
      "Epoch 475/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0157 - acc: 0.9955 - val_loss: 0.5067 - val_acc: 0.9260\n",
      "Epoch 476/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0140 - acc: 0.9958 - val_loss: 0.5136 - val_acc: 0.9248\n",
      "Epoch 477/600\n",
      "6693/6693 [==============================] - 1s 115us/sample - loss: 0.0125 - acc: 0.9967 - val_loss: 0.4784 - val_acc: 0.9257\n",
      "Epoch 478/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0089 - acc: 0.9970 - val_loss: 0.4688 - val_acc: 0.9296\n",
      "Epoch 479/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0070 - acc: 0.9970 - val_loss: 0.4615 - val_acc: 0.9278\n",
      "Epoch 480/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0124 - acc: 0.9961 - val_loss: 0.4954 - val_acc: 0.9257\n",
      "Epoch 481/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0117 - acc: 0.9955 - val_loss: 0.5426 - val_acc: 0.9260\n",
      "Epoch 482/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0225 - acc: 0.9948 - val_loss: 0.5313 - val_acc: 0.9254\n",
      "Epoch 483/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0087 - acc: 0.9957 - val_loss: 0.5187 - val_acc: 0.9245\n",
      "Epoch 484/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0087 - acc: 0.9967 - val_loss: 0.5202 - val_acc: 0.9302\n",
      "Epoch 485/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0129 - acc: 0.9964 - val_loss: 0.5484 - val_acc: 0.9257\n",
      "Epoch 486/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0123 - acc: 0.9961 - val_loss: 0.5790 - val_acc: 0.9224\n",
      "Epoch 487/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0157 - acc: 0.9960 - val_loss: 0.5214 - val_acc: 0.9272\n",
      "Epoch 488/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0080 - acc: 0.9970 - val_loss: 0.5098 - val_acc: 0.9290\n",
      "Epoch 489/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0085 - acc: 0.9973 - val_loss: 0.5226 - val_acc: 0.9299\n",
      "Epoch 490/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0164 - acc: 0.9945 - val_loss: 0.5338 - val_acc: 0.9211\n",
      "Epoch 491/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0129 - acc: 0.9955 - val_loss: 0.5349 - val_acc: 0.9242\n",
      "Epoch 492/600\n",
      "6693/6693 [==============================] - 1s 109us/sample - loss: 0.0100 - acc: 0.9969 - val_loss: 0.5587 - val_acc: 0.9290\n",
      "Epoch 493/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0084 - acc: 0.9973 - val_loss: 0.5406 - val_acc: 0.9336\n",
      "Epoch 494/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0152 - acc: 0.9952 - val_loss: 0.5421 - val_acc: 0.9224\n",
      "Epoch 495/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0200 - acc: 0.9960 - val_loss: 0.5768 - val_acc: 0.9224\n",
      "Epoch 496/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0248 - acc: 0.9936 - val_loss: 0.4915 - val_acc: 0.9242\n",
      "Epoch 497/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0195 - acc: 0.9945 - val_loss: 0.5225 - val_acc: 0.9227\n",
      "Epoch 498/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0153 - acc: 0.9963 - val_loss: 0.4900 - val_acc: 0.9251\n",
      "Epoch 499/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0102 - acc: 0.9966 - val_loss: 0.5234 - val_acc: 0.9281\n",
      "Epoch 500/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0105 - acc: 0.9964 - val_loss: 0.4993 - val_acc: 0.9275\n",
      "Epoch 501/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0125 - acc: 0.9957 - val_loss: 0.5000 - val_acc: 0.9308\n",
      "Epoch 502/600\n",
      "6693/6693 [==============================] - 1s 110us/sample - loss: 0.0106 - acc: 0.9967 - val_loss: 0.5039 - val_acc: 0.9287\n",
      "Epoch 503/600\n",
      "6693/6693 [==============================] - 1s 108us/sample - loss: 0.0147 - acc: 0.9960 - val_loss: 0.4801 - val_acc: 0.9263\n",
      "Epoch 504/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0077 - acc: 0.9970 - val_loss: 0.5140 - val_acc: 0.9269\n",
      "Epoch 505/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0121 - acc: 0.9957 - val_loss: 0.4638 - val_acc: 0.9290\n",
      "Epoch 506/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0096 - acc: 0.9963 - val_loss: 0.5013 - val_acc: 0.9257\n",
      "Epoch 507/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0085 - acc: 0.9970 - val_loss: 0.5392 - val_acc: 0.9260\n",
      "Epoch 508/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0073 - acc: 0.9970 - val_loss: 0.5303 - val_acc: 0.9251\n",
      "Epoch 509/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0102 - acc: 0.9960 - val_loss: 0.5323 - val_acc: 0.9248\n",
      "Epoch 510/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0078 - acc: 0.9976 - val_loss: 0.5078 - val_acc: 0.9305\n",
      "Epoch 511/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0153 - acc: 0.9961 - val_loss: 0.5644 - val_acc: 0.9263\n",
      "Epoch 512/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0166 - acc: 0.9946 - val_loss: 0.5127 - val_acc: 0.9245\n",
      "Epoch 513/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0082 - acc: 0.9975 - val_loss: 0.4807 - val_acc: 0.9287\n",
      "Epoch 514/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0126 - acc: 0.9958 - val_loss: 0.4899 - val_acc: 0.9269\n",
      "Epoch 515/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0106 - acc: 0.9970 - val_loss: 0.4838 - val_acc: 0.9302\n",
      "Epoch 516/600\n",
      "6693/6693 [==============================] - 1s 113us/sample - loss: 0.0189 - acc: 0.9946 - val_loss: 0.4543 - val_acc: 0.9305\n",
      "Epoch 517/600\n",
      "6693/6693 [==============================] - 1s 109us/sample - loss: 0.0154 - acc: 0.9960 - val_loss: 0.4745 - val_acc: 0.9260\n",
      "Epoch 518/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0139 - acc: 0.9960 - val_loss: 0.4211 - val_acc: 0.9302\n",
      "Epoch 519/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0119 - acc: 0.9949 - val_loss: 0.4990 - val_acc: 0.9278\n",
      "Epoch 520/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0091 - acc: 0.9966 - val_loss: 0.4683 - val_acc: 0.9296\n",
      "Epoch 521/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0080 - acc: 0.9966 - val_loss: 0.4825 - val_acc: 0.9302\n",
      "Epoch 522/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0110 - acc: 0.9957 - val_loss: 0.5058 - val_acc: 0.9287\n",
      "Epoch 523/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0131 - acc: 0.9955 - val_loss: 0.5418 - val_acc: 0.9245\n",
      "Epoch 524/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0189 - acc: 0.9943 - val_loss: 0.4998 - val_acc: 0.9251\n",
      "Epoch 525/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0083 - acc: 0.9966 - val_loss: 0.4740 - val_acc: 0.9230\n",
      "Epoch 526/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0146 - acc: 0.9955 - val_loss: 0.4880 - val_acc: 0.9263\n",
      "Epoch 527/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0103 - acc: 0.9964 - val_loss: 0.5280 - val_acc: 0.9202\n",
      "Epoch 528/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0139 - acc: 0.9957 - val_loss: 0.5522 - val_acc: 0.9217\n",
      "Epoch 529/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0151 - acc: 0.9961 - val_loss: 0.5419 - val_acc: 0.9233\n",
      "Epoch 530/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0167 - acc: 0.9943 - val_loss: 0.5030 - val_acc: 0.9242\n",
      "Epoch 531/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0143 - acc: 0.9955 - val_loss: 0.5001 - val_acc: 0.9245\n",
      "Epoch 532/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0107 - acc: 0.9966 - val_loss: 0.5118 - val_acc: 0.9266\n",
      "Epoch 533/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0073 - acc: 0.9976 - val_loss: 0.5007 - val_acc: 0.9281\n",
      "Epoch 534/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 0.5248 - val_acc: 0.9221\n",
      "Epoch 535/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0094 - acc: 0.9963 - val_loss: 0.5385 - val_acc: 0.9236\n",
      "Epoch 536/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0112 - acc: 0.9972 - val_loss: 0.5256 - val_acc: 0.9248\n",
      "Epoch 537/600\n",
      "6693/6693 [==============================] - 1s 105us/sample - loss: 0.0097 - acc: 0.9958 - val_loss: 0.5741 - val_acc: 0.9227\n",
      "Epoch 538/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0139 - acc: 0.9949 - val_loss: 0.5433 - val_acc: 0.9275\n",
      "Epoch 539/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0131 - acc: 0.9963 - val_loss: 0.5470 - val_acc: 0.9251\n",
      "Epoch 540/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0159 - acc: 0.9957 - val_loss: 0.5679 - val_acc: 0.9251\n",
      "Epoch 541/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0170 - acc: 0.9945 - val_loss: 0.5535 - val_acc: 0.9211\n",
      "Epoch 542/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0142 - acc: 0.9955 - val_loss: 0.5271 - val_acc: 0.9196\n",
      "Epoch 543/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0119 - acc: 0.9963 - val_loss: 0.5251 - val_acc: 0.9214\n",
      "Epoch 544/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0074 - acc: 0.9969 - val_loss: 0.5213 - val_acc: 0.9233\n",
      "Epoch 545/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0100 - acc: 0.9966 - val_loss: 0.5255 - val_acc: 0.9236\n",
      "Epoch 546/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0167 - acc: 0.9957 - val_loss: 0.5277 - val_acc: 0.9214\n",
      "Epoch 547/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0122 - acc: 0.9946 - val_loss: 0.5516 - val_acc: 0.9224\n",
      "Epoch 548/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0127 - acc: 0.9960 - val_loss: 0.5459 - val_acc: 0.9227\n",
      "Epoch 549/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0189 - acc: 0.9951 - val_loss: 0.5355 - val_acc: 0.9242\n",
      "Epoch 550/600\n",
      "6693/6693 [==============================] - 1s 115us/sample - loss: 0.0145 - acc: 0.9948 - val_loss: 0.5281 - val_acc: 0.9227\n",
      "Epoch 551/600\n",
      "6693/6693 [==============================] - 1s 118us/sample - loss: 0.0091 - acc: 0.9970 - val_loss: 0.5172 - val_acc: 0.9239\n",
      "Epoch 552/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0103 - acc: 0.9963 - val_loss: 0.5264 - val_acc: 0.9242\n",
      "Epoch 553/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0074 - acc: 0.9976 - val_loss: 0.5272 - val_acc: 0.9260\n",
      "Epoch 554/600\n",
      "6693/6693 [==============================] - 1s 101us/sample - loss: 0.0095 - acc: 0.9966 - val_loss: 0.5057 - val_acc: 0.9248\n",
      "Epoch 555/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0093 - acc: 0.9966 - val_loss: 0.5225 - val_acc: 0.9251\n",
      "Epoch 556/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0125 - acc: 0.9946 - val_loss: 0.5241 - val_acc: 0.9242\n",
      "Epoch 557/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0067 - acc: 0.9976 - val_loss: 0.5236 - val_acc: 0.9287\n",
      "Epoch 558/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0086 - acc: 0.9970 - val_loss: 0.5269 - val_acc: 0.9266\n",
      "Epoch 559/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0070 - acc: 0.9975 - val_loss: 0.5054 - val_acc: 0.9315\n",
      "Epoch 560/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0086 - acc: 0.9957 - val_loss: 0.5568 - val_acc: 0.9266\n",
      "Epoch 561/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0040 - acc: 0.9988 - val_loss: 0.5281 - val_acc: 0.9278\n",
      "Epoch 562/600\n",
      "6693/6693 [==============================] - 1s 112us/sample - loss: 0.0119 - acc: 0.9969 - val_loss: 0.5418 - val_acc: 0.9242\n",
      "Epoch 563/600\n",
      "6693/6693 [==============================] - 1s 104us/sample - loss: 0.0111 - acc: 0.9966 - val_loss: 0.5397 - val_acc: 0.9236\n",
      "Epoch 564/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0180 - acc: 0.9945 - val_loss: 0.4921 - val_acc: 0.9296\n",
      "Epoch 565/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0112 - acc: 0.9960 - val_loss: 0.4440 - val_acc: 0.9284\n",
      "Epoch 566/600\n",
      "6693/6693 [==============================] - 1s 114us/sample - loss: 0.0081 - acc: 0.9973 - val_loss: 0.4607 - val_acc: 0.9290\n",
      "Epoch 567/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0105 - acc: 0.9966 - val_loss: 0.4668 - val_acc: 0.9284\n",
      "Epoch 568/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0104 - acc: 0.9964 - val_loss: 0.4647 - val_acc: 0.9278\n",
      "Epoch 569/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0179 - acc: 0.9963 - val_loss: 0.4544 - val_acc: 0.9260\n",
      "Epoch 570/600\n",
      "6693/6693 [==============================] - 1s 97us/sample - loss: 0.0086 - acc: 0.9969 - val_loss: 0.4576 - val_acc: 0.9302\n",
      "Epoch 571/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0090 - acc: 0.9969 - val_loss: 0.4747 - val_acc: 0.9302\n",
      "Epoch 572/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0083 - acc: 0.9967 - val_loss: 0.4757 - val_acc: 0.9287\n",
      "Epoch 573/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0092 - acc: 0.9963 - val_loss: 0.5166 - val_acc: 0.9308\n",
      "Epoch 574/600\n",
      "6693/6693 [==============================] - 1s 107us/sample - loss: 0.0137 - acc: 0.9963 - val_loss: 0.4810 - val_acc: 0.9299\n",
      "Epoch 575/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0087 - acc: 0.9972 - val_loss: 0.4921 - val_acc: 0.9290\n",
      "Epoch 576/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0109 - acc: 0.9963 - val_loss: 0.5674 - val_acc: 0.9217\n",
      "Epoch 577/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0167 - acc: 0.9948 - val_loss: 0.5232 - val_acc: 0.9293\n",
      "Epoch 578/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0123 - acc: 0.9955 - val_loss: 0.5101 - val_acc: 0.9281\n",
      "Epoch 579/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0140 - acc: 0.9961 - val_loss: 0.5254 - val_acc: 0.9224\n",
      "Epoch 580/600\n",
      "6693/6693 [==============================] - 1s 105us/sample - loss: 0.0124 - acc: 0.9949 - val_loss: 0.5560 - val_acc: 0.9205\n",
      "Epoch 581/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0125 - acc: 0.9960 - val_loss: 0.5190 - val_acc: 0.9245\n",
      "Epoch 582/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0168 - acc: 0.9957 - val_loss: 0.5552 - val_acc: 0.9217\n",
      "Epoch 583/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0115 - acc: 0.9966 - val_loss: 0.5042 - val_acc: 0.9281\n",
      "Epoch 584/600\n",
      "6693/6693 [==============================] - 1s 110us/sample - loss: 0.0112 - acc: 0.9963 - val_loss: 0.5464 - val_acc: 0.9242\n",
      "Epoch 585/600\n",
      "6693/6693 [==============================] - 1s 111us/sample - loss: 0.0195 - acc: 0.9951 - val_loss: 0.5057 - val_acc: 0.9263\n",
      "Epoch 586/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0279 - acc: 0.9903 - val_loss: 0.4875 - val_acc: 0.9266\n",
      "Epoch 587/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0162 - acc: 0.9945 - val_loss: 0.4730 - val_acc: 0.9254\n",
      "Epoch 588/600\n",
      "6693/6693 [==============================] - 1s 103us/sample - loss: 0.0149 - acc: 0.9967 - val_loss: 0.4816 - val_acc: 0.9230\n",
      "Epoch 589/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0109 - acc: 0.9967 - val_loss: 0.4724 - val_acc: 0.9305\n",
      "Epoch 590/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0163 - acc: 0.9964 - val_loss: 0.4523 - val_acc: 0.9296\n",
      "Epoch 591/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0086 - acc: 0.9967 - val_loss: 0.4631 - val_acc: 0.9278\n",
      "Epoch 592/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0079 - acc: 0.9973 - val_loss: 0.4798 - val_acc: 0.9269\n",
      "Epoch 593/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0062 - acc: 0.9973 - val_loss: 0.4575 - val_acc: 0.9321\n",
      "Epoch 594/600\n",
      "6693/6693 [==============================] - 1s 98us/sample - loss: 0.0058 - acc: 0.9975 - val_loss: 0.4913 - val_acc: 0.9290\n",
      "Epoch 595/600\n",
      "6693/6693 [==============================] - 1s 102us/sample - loss: 0.0128 - acc: 0.9963 - val_loss: 0.4917 - val_acc: 0.9318\n",
      "Epoch 596/600\n",
      "6693/6693 [==============================] - 1s 113us/sample - loss: 0.0141 - acc: 0.9958 - val_loss: 0.4943 - val_acc: 0.9281\n",
      "Epoch 597/600\n",
      "6693/6693 [==============================] - 1s 105us/sample - loss: 0.0086 - acc: 0.9969 - val_loss: 0.4824 - val_acc: 0.9311\n",
      "Epoch 598/600\n",
      "6693/6693 [==============================] - 1s 99us/sample - loss: 0.0076 - acc: 0.9964 - val_loss: 0.5185 - val_acc: 0.9248\n",
      "Epoch 599/600\n",
      "6693/6693 [==============================] - 1s 108us/sample - loss: 0.0096 - acc: 0.9973 - val_loss: 0.4943 - val_acc: 0.9269\n",
      "Epoch 600/600\n",
      "6693/6693 [==============================] - 1s 100us/sample - loss: 0.0094 - acc: 0.9975 - val_loss: 0.5206 - val_acc: 0.9254\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=128)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3297/3297 [==============================] - 0s 29us/sample - loss: 0.5206 - acc: 0.9254\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('The test loss is: ', test_loss)\r\n",
    "print('The Best test Accuracy is: ', test_acc*100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The test loss is:  0.5205980658187699\n",
      "The Best test Accuracy is:  92.53867268562317\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Other ML models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "categorylist = list(df.select_dtypes(['object']))\r\n",
    "tempdf = pd.get_dummies(df, columns=categorylist)\r\n",
    "label_columns = [x for x in tempdf.columns if x.startswith('label')]\r\n",
    "X = tempdf.drop(label_columns, axis=1)\r\n",
    "y = tempdf[label_columns].copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "encoder = LabelEncoder()\r\n",
    "categorylist = list(df.select_dtypes(['object']))\r\n",
    "y = df[categorylist]\r\n",
    "y_encoded = encoder.fit_transform(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Sam\\miniconda3\\envs\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "class OldDataFrameSelector(BaseEstimator, TransformerMixin):\r\n",
    "    def __init__(self, attribute_names):\r\n",
    "        self.attribute_names = attribute_names\r\n",
    "    def fit(self, X, y=None):\r\n",
    "        return self\r\n",
    "    def transform(self, X):\r\n",
    "        return X[self.attribute_names].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "def from_multilabel_to_onehot(array, column='label', return_columns=False):\r\n",
    "    df = pd.DataFrame(y_test, columns=[column])\r\n",
    "    df = pd.get_dummies(df, columns=[column])\r\n",
    "    \r\n",
    "    if return_columns:\r\n",
    "        return [x.replace('label_','') for x in df.columns]\r\n",
    "    return df.to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "num_attribs = list(X.select_dtypes(['int64', 'float64']))\r\n",
    "pipeline = Pipeline([\r\n",
    "    ('selector', OldDataFrameSelector(num_attribs)),\r\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\r\n",
    "    ('std_scaler', StandardScaler()),\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "X_prepared = pipeline.fit_transform(X)\r\n",
    "# y_prepared = y.to_numpy()[:,1]\r\n",
    "y_prepared = y.to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=10, test_size= 0.33, random_state=0)\r\n",
    "\r\n",
    "classifiers = [\r\n",
    "    KNeighborsClassifier(),\r\n",
    "    ExtraTreeClassifier(),\r\n",
    "    DecisionTreeClassifier(),\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "res = []\r\n",
    "labels = [x.replace('label_','') for x in y.columns]\r\n",
    "for train_idx, test_idx in sss.split(X_prepared, y_prepared):\r\n",
    "    X_train, X_test = X_prepared[train_idx], X_prepared[test_idx]\r\n",
    "    y_train, y_test = y_prepared[train_idx], y_prepared[test_idx]\r\n",
    "    for clf in classifiers:\r\n",
    "        name = clf.__class__.__name__\r\n",
    "        clf.fit(X_train, y_train)\r\n",
    "        y_pred = clf.predict(X_test)\r\n",
    "        y_pred_probas = clf.predict_proba(X_test)\r\n",
    "        acc = accuracy_score(y_test, y_pred)\r\n",
    "        loss = log_loss(y_test, y_pred)\r\n",
    "        \r\n",
    "        # Compute ROC curve and ROC area for each class\r\n",
    "        fpr = dict()\r\n",
    "        tpr = dict()\r\n",
    "        roc_auc = dict()\r\n",
    "        for i, label in enumerate(labels):\r\n",
    "            fpr[label], tpr[label], _ = roc_curve(y_test[:,i], y_pred_probas[i][:,1])\r\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\r\n",
    "            res.append([name, label, acc, loss, roc_auc[label]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "res_svc = []\r\n",
    "clf = SVC(probability=True)\r\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(sss.split(X_prepared, y_encoded)):\r\n",
    "    print(fold_idx)\r\n",
    "    X_train, X_test = X_prepared[train_idx], X_prepared[test_idx]\r\n",
    "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\r\n",
    "    name = clf.__class__.__name__\r\n",
    "    clf.fit(X_train, y_train)\r\n",
    "    y_pred = clf.predict(X_test)\r\n",
    "    y_pred_probas = clf.predict_proba(X_test)\r\n",
    "    acc = accuracy_score(y_test, y_pred)\r\n",
    "    loss = log_loss(y_test, y_pred_probas)\r\n",
    "    \r\n",
    "    labels = from_multilabel_to_onehot(y_test, return_columns=True)\r\n",
    "    y_test = from_multilabel_to_onehot(y_test)\r\n",
    "    # y_pred_probas = from_multilabel_to_onehot(y_pred_probas)\r\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\r\n",
    "    for i, label in enumerate(labels):\r\n",
    "        fpr[label], tpr[label], _ = roc_curve(y_test[:,i], y_pred_probas[:,i])\r\n",
    "        roc_auc[label] = auc(fpr[label], tpr[label])\r\n",
    "        res_svc.append([name, label, acc, loss, roc_auc[label]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "log_svc = pd.DataFrame(res_svc, columns=['Classifier','Label', 'Accuracy','Log Loss','AUC'])\r\n",
    "log = pd.DataFrame(res, columns=['Classifier','Label', 'Accuracy','Log Loss','AUC'])\r\n",
    "a = log_svc.groupby(['Classifier'], as_index=False).mean()\r\n",
    "b = log.groupby(['Classifier'], as_index=False).mean()\r\n",
    "pd.concat([b,a],axis=0).sort_values(by='Accuracy', ascending=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.848286</td>\n",
       "      <td>0.457128</td>\n",
       "      <td>0.986214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.833910</td>\n",
       "      <td>2.898170</td>\n",
       "      <td>0.986842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.626630</td>\n",
       "      <td>12.875201</td>\n",
       "      <td>0.772335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>0.529148</td>\n",
       "      <td>16.248974</td>\n",
       "      <td>0.714333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Classifier  Accuracy   Log Loss       AUC\n",
       "0                     SVC  0.848286   0.457128  0.986214\n",
       "2    KNeighborsClassifier  0.833910   2.898170  0.986842\n",
       "0  DecisionTreeClassifier  0.626630  12.875201  0.772335\n",
       "1     ExtraTreeClassifier  0.529148  16.248974  0.714333"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('venv': conda)"
  },
  "interpreter": {
   "hash": "1770bc253cc25a4f6117d11136c6fe1ae98e60103fb41d13511f1749add39a66"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}